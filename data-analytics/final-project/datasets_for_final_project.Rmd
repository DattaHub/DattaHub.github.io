---
title: "Datasets: Sparse Regression or Classification"
author: "Jyotishka Datta"
date: "`r Sys.Date()`"
output: 
  html_document:
      toc: true
      number_sections: false
      toc_float: true
fontsize: 11pt
geometry: margin=1in
urlcolor: blue
theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

I have described how to download and read two high-dimensional datasets, one for regression and one for classification and also point to a few R packages with extensions of Lasso-like methods for handling high-dim datasets. Feel free to use them if you want. I have also pasted some other sources for finding interesting data-sets below where you can find more such data-sets. 


-  UCI ML repository (good source for high-dimensional data) https://archive.ics.uci.edu/ml/index.php 
- Kaggle.com has really interesting data-sets, too. For example, you can find cool data sets for classification here: https://www.kaggle.com/datasets?tags=13302-Classification 
- `library(datasets)` in R.  
-  TIDYTUESDAY datasets: (I love this) https://github.com/rfordatascience/tidytuesday#datasets
-  FiveThirtyEight datasets: (a little contextual reading might be required) https://github.com/fivethirtyeight/data


## R packages for extensions of Lasso-like ethods 

Some variants of Lasso and other methods for high-dimensional inference. It will be a great idea to learn how to use one of these packages for a high-dimensional data-set and compare one of these more advanced methods wih Lasso. 

1. Minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD) penalty. [R package vignette](https://cran.r-project.org/web/packages/ncvreg/vignettes/getting-started.html) and [main webpage](https://pbreheny.github.io/ncvreg/)

2. FLARE: Family of Lasso Regression: Includes Dantzig Selector, LAD Lasso, SQRT Lasso, Lq Lasso: [Vignette](https://cran.r-project.org/web/packages/flare/vignettes/vignette.pdf) and [main webpage]() https://cran.r-project.org/web/packages/flare/index.html).

3. sparsenet: Fit Sparse Linear Regression Models via Nonconvex Optimization. [main webpage](https://cran.r-project.org/web/packages/sparsenet/index.html)

4. horseshoe: Bayesian method for high-dimensional regression [Vignette](https://cran.r-project.org/web/packages/horseshoe/vignettes/horseshoe-vignette.html) and [main page](https://cran.r-project.org/web/packages/horseshoe/index.html)



## Examples 

Now, we will read two different data-sets: one called `Scheetz2006` and the other `Golub1999`, named after the papers that introduced them to the world. This little demo will show you how to install the HDRM package and download the data-sets as well as how to split them into training and test, but I have also uploaded a bunch of RData files on Canvas (note that they're different types of dataset there) if you want to skip this and directly read them. 



## Install HDRM package (source of data)

**Warning: the installation of packages and libraries will take some time when you run this for the first time ever. Then you can safely change the flag so that it does not install the package every time you run the R codes.**

```{r, echo = T, cache = T, message=FALSE, warning=FALSE}

installed_flag = T

if(installed_flag == F){
  if(!require(remotes)){
  install.packages("remotes", dependencies = TRUE, repos = 'http://cran.rstudio.com')
 }
remotes::install_github("pbreheny/hdrm")
}
```


## Sparse Regression 

### Comparing penalized methods for High-dimensional Regression

We would like to compare the performance of several modern regression methods including penalized regression methods such as LASSO and Elastic Net regression with 10-fold cross-validation. 

### Mammalian Eye Data

We will illustrate the methods using a high-dimensional data-set that consists of gene expression in the mammalian eye from (Scheetz, 2006). This data-set has 120 observations for 18,976 variables (predictors): a typical $p \gg n$ data-set where the usual regression methods do not work because the $X^TX$ matrix is not invertible anymore. 

Also, in such cases, it is natural to ask: which of these many variables ($X_j$'s) are important for predicting $y$?

For the original analysis, microarrays were used to measure levels of RNA expression in the isolated eye tissues extracted from 120 laboratory inbred rats from F2 generation. 18,976 probes were detected to be considered as mammalian eye. In the data, we treat gene Trim32 as the outcome(y) since it is known to be linked with a genetic disorder called BBS and remaining gene expression measurements are x.

```{r, echo = T, cache = T, message=FALSE, warning=FALSE}
library(hdrm)
downloadData(Scheetz2006)
attachData(Scheetz2006)

n = nrow(X)
# Split data into train and test sets
train_rows <- sample(1:n, n/2)
X.train <- X[train_rows, ]
X.test <- X[-train_rows, ]

y.train <- y[train_rows]
y.test <- y[-train_rows]

dim(X.train)
dim(X.test)

length(y.train)

length(y.test)
```


## Sparse Classification 

### Comparing penalized methods for High-dimensional Classification

We would like to compare the performance of several modern regression methods including penalized regression methods such as LASSO and Elastic Net regression with 10-fold cross-validation. 

### Golub 1999 Data 

This data set consists of 47 patients with acute lymphoblastic leukemia (ALL) and 25 patients with acute myeloid leukemia (AML). Each of the 72 patients had bone marrow samples obtained at the time of diagnosis. Gene expression measurements for the samples were taken using Affymetrix Hgu6800 chips, resulting in 7129 measurements per patient.

Of the two diseases, AML has a considerably worse prognosis: only 26% survive at least 5 years following diagnosis, compared to 68% for ALL.



```{r, echo = T, cache = T, message=FALSE, warning=FALSE}

library(hdrm)
downloadData(Golub1999)
attachData(Golub1999)

n = nrow(X)
# Split data into train and test sets
train_rows <- sample(1:n, n/2)
X.train <- X[train_rows, ]
X.test <- X[-train_rows, ]

y.train <- y[train_rows]
y.test <- y[-train_rows]

dim(X.train)
dim(X.test)

length(y.train)

length(y.test)
```


