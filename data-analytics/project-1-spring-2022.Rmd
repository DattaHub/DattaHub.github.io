---
title: "Project 1 Description"
subtitle: "Regression"
author: "Dr. Jyotishka Datta"
date: "`r Sys.Date()`"
output: 
  html_document:
      toc: true
      number_sections: false
      toc_float: true
      theme: united
fontsize: 12pt
geometry: margin=1in
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

# Project 1: Supervised Learning: Sparse Regression 

The goal of this project is to apply supervised learning to a high-dimensional regression problem. Our goal would be to compare between multiple modern regression approaches (e.g. Stepwise, Lasso, Ridge, Elastic Net, PCR or others) for the same inferential task. 

The project goals include the following:

1.  Understand how to handle sparse regression problem. 
2.  use R packages for comparing at least *three* different methods. 
3.  Perform supervised learning with a real data-set. 

I will discuss how a supervised sparse regression method like Lasso works in details in class, and talk about some of the improvements since Lasso. We have also done at least one simulation study involving Lasso in our computer lab. 
## Example 1: HIV Data (Supervised)

A PNAS paper studied in-vitro drug resistance of $n = 1057$ HIV-1 isolates to protease and reverse transcriptase mutations. There are $p = 208$ (binary) mutation variables. The original paper compares 5 different regression methods: decision trees, neural networks, SVM regression, OLS and LASSO. 

You can read the full paper (if you're interested): [http://www.pnas.org/content/103/46/17355.full](http://www.pnas.org/content/103/46/17355.full)

Here our goal would be to understand how to apply Lasso on this data-set and what do the outputs mean: 

The first step is to load the data-set. Because this is a supervised learning problem, we have a training data that is labelled and a spearate test data where we test the performance of our methods. 

```{r, echo = T, tidy = T, cache=T}
setwd("C:/Users/jyotishka/OneDrive/Documents/Course Notes/data_analytics/2022/R codes/datasets")
## save the hiv.rda file somewhere that R can find and use that path
load("hiv.rda")
dim(hiv.train$x)
dim(hiv.test$x)
```

### Lasso using `glmnet' package:

The following snippet shows how to fit a Lasso regression model to the HIV data using the `glmnet` package. The `cv.glmnet` function performs a $k$-fold cross validation to choose the tuning parameter $\lambda$ for a user-supplied value of $k$, the default is $k = 10$. 

```{r, echo = T, warning = F, message = F, cache=T, fig.asp = 0.5}
require(glmnet)
set.seed(1)
cvfit <- cv.glmnet(hiv.train$x, hiv.train$y)
fit <- cvfit$glmnet.fit
```

The solution path tells us how the coefficients enter (or leave) the model as we decrease (or increase) the penalty $\lambda$. The second plot shows the solution pat for a restricted X-axis. 

```{r, echo = T, warning = F, message = F, cache=T, fig.asp = 0.6}
plot(fit, xvar = "lambda")
xlim <- log(c(fit$lambda[1], cvfit$lambda.min))
plot(fit, xlim=xlim, xvar="lambda")
```


The next plot shows the mean squared error for Lasso plotted against $\lambda$ values used. The numbers on the top margin of the plot tells us the number of variables selected at that value of $\lambda$. 

```{r, echo = T, warning = F, message = F, cache=T, fig.asp = 0.5}
plot(cvfit)
```

### Variable selection by Lasso

One of the main advantages of Lasso, as discussed in class, is that it can automatically shrink many coefficients to zero - resulting in a sparse model. 

```{r}
lasso.mod = glmnet(hiv.train$x, hiv.train$y,alpha = 1)
bestlam = cvfit$lambda.min
lasso.coef = predict(lasso.mod,type ="coefficients",s=bestlam)
lasso_cv_est = lasso.coef[-1]
lasso.ind <- ((lasso_cv_est!=0))
```

```{r, echo = T}
cat("No of variables selected", sum(lasso.ind), "\n")
cat("Estimates for selected coefficients \n", lasso_cv_est[lasso.ind], "\n")
cat("Which columns were selected?", which(lasso.ind == 1))
```

The plot of estimated coefficients show the sparsity inducing property of Lasso:

```{r}
plot(lasso_cv_est, pch = 4, col = 4, xlab = "Indices", ylab = "Estimates")
```

The mean squred error can calculated as follows: 

```{r, echo = T}
lasso.pred=predict(fit, s = bestlam , newx = hiv.test$x)
mean((lasso.pred - hiv.test$y)^2)
```

# Inferential Goals for this project 

Now your goal would be to compare a few other popular regularization methods or Bayesian methods for this data and compare their performance. You don't need to implement any regularization method or dimension reduction strategies on your own. You can use R packages available freely for this project. 

1.  Apply different sparse regression (regularization/shrinkage/dimension reduction technique) on this data. You can choose any 3 methods covered in class or outside. Here is a short list of some of the popular regularization methods we have not covered in class. It is not mandatory to use them but using a different/new tool is always an advantage. 

| **Method **                                                                                                                                                                                      | **R   demo/example/package **                                                       |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------|
| [Adaptive   Lasso](https://pages.cs.wisc.edu/~shao/stat992/zou2006.pdf)                                                                                                                          | https://ricardocarvalho.ca/post/lasso/                                              |
| [Dantzig   selector](https://projecteuclid.org/journals/annals-of-statistics/volume-35/issue-6/The-Dantzig-selector--Statistical-estimation-when-p-is-much/10.1214/009053606000001523.full)      | https://cran.r-project.org/web/packages/fastclime/                                  |
| [Square Root   Lasso](https://arxiv.org/abs/1009.5689)                                                                                                                                           | [flare   package](https://www.jmlr.org/papers/volume16/li15a/li15a.pdf)             |
| [Minimum   Concave   Penalty](https://projecteuclid.org/journals/annals-of-statistics/volume-38/issue-2/Nearly-unbiased-variable-selection-under-minimax-concave-penalty/10.1214/09-AOS729.full) | [plus   package](https://cran.r-project.org/web/packages/plus/)                     |
| [Sparsenet](https://hastie.su.domains/Papers/Sparsenet/jasa_MFH_final.pdf)                                                                                                                       | [sparsenet   package](https://cran.r-project.org/web/packages/sparsenet/index.html) |



2. You can also look at able 1 and 2 of the excellent review paper: ['In praise of sparsity and convexity", Tibshirani (2014)](https://tibshirani.su.domains/ftp/tibs-copss.pdf) by the author of Lasso Robert Tibshirani for a list of popular regularization methods. All of these methods are designed for sparse systems. 

2. Compare between different methods? e.g. Would Elastic Net (or MCP or Dantzig Selector) be a better fit than Lasso? Do concave methods outperform convex regularizing methods? How do you test? How about Bayesian methods or PC Regression? Choose any *three* methods that you like. 




# Appendix: Bayesian alternatives 

## Horseshoe 

We have not talked much about Bayesian methods for handling sparse regression such as Bayesian Lasso and horseshoe in class. The main idea is that we can interpret the penalty as a prior on the coefficients in a Bayesian set up, e.g. the Lasso penalty $\ell_1$ norm $\sum_j |\beta_j|$ can be interpreted in a Bayesian set-up as a double exponential or Laplace prior density on $\beta_j$ like $p(\beta_j) \propto \exp\{-|\beta_j|\}$. There are, of course, other motivations of Bayesian estimators for high-dimensional data and most of the successful methods are derived in a natural way, i.e. in a truely Bayesian fashion. For example, the [spike and slab prior](https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478694) puts a 'spike' at zero for capturing sparsity and a 'slab' that explains the non-zero signals. This is a very active and well-developed area with lots of development but we won't get into this here. 

Just as an example of what Bayesian methods look like, I will briefly describe the horseshoe prior, proposed by [Carvalho, Polson and Scott (2010)](http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf) for sparse signal recovery here. Horseshoe prior improves on the Laplace prior used for Bayesian Lasso when the underlying truth is sparse. The horseshoe prior is given by the following hierarchical model: 

$$
\begin{align}
Y_i \mid \beta_i & \sim N(X\beta, \sigma^2I); \\
\beta_i & \mid \lambda_i \sim N\left ( 0 , \lambda_i^2 \tau^2 \right ); \\
\underbrace{\lambda_i^2}_{\text{local}} \mid \tau & \sim C^{+}(0, \tau), \underbrace{\tau}_{\text{global}} \sim C^{+}(0, \sigma) \; \; (\text{Heavy-tailed prior})
\end{align}
$$

As we have seen before, here $\lambda_i$ term is called a local shrinkage parameter as it helps in tagging the signals, $\tau$ is called a global shrinkage parameter as it adjusts to the overall sparsity in the data. 

There are several R packages for implementing horseshoe regression: such as  `horseshoe`, `monomvn`, `bayeslm` etc. We will demonstrate one of them below: 

### Horseshoe R package {.smaller}

Here we apply the monomvn package, with a truncated Cauchy prior on the global srinkage parameter $\tau$ and a Jeffreys' prior on the variance parameter $\sigma$. 

```{r, echo = T}
library(horseshoe)
hs.fit <- horseshoe(y = hiv.train$y, X = hiv.train$x, 
                    method.tau = "truncatedCauchy", method.sigma = "Jeffreys",
                    burn = 5000, nmc = 10000, thin = 2)
```

First we plot the estimated coefficients - only a few are away from zero as expected. We plot the horseshoe and Lasso side-by-side to show how similar they are: 

```{r, echo = T}
par(mfrow=c(1,2))
plot(hs.fit$BetaHat, main = "Horseshoe", ylab = "Estimates")
plot(lasso_cv_est, main = "Lasso", ylab = "Estimates")
```


### Which method performs better? 

-  In absence of "ground truth" about the predictors, it is hard to tell if horseshoe is over-penalizing or Lasso is severely under-penalizing. 
-  That's why we choose methods based on a simulation study that we think accurately represents our reality (hard!)


### Variable Selection by Horseshoe

```{r, echo = T}
hs.ind <- HS.var.select(hs.fit, hiv.train$y, method = "interval") 
```

```{r, echo = T}
cat("No of variables selected", sum(hs.ind), "\n")
cat("Estimates for selected coefficients \n", hs.fit$BetaHat[hs.ind], "\n")
cat("Which columns were selected?", which(hs.ind == 1))
```

We can compare these numbers with Lasso's output above. Now answer these quesions by yourself. 

1.  How many variables did Lasso and Horseshoe select? 
2.  Which of the two methods leads to a sparser solution? 
3.  Can you guess the reason? 

Finally, the mean squarred error can be calcuated as follows: 

```{r}
hs.pred <- hiv.test$x%*%hs.fit$BetaHat
(MSE = mean((hiv.test$y - hs.pred)^2))
```

Again, the same questions can be asked. 
