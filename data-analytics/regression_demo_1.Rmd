---
title: "Modern Regression"
author: "Jyotishka Datta"
date: "`r Sys.Date()`"
output: 
  ioslides_presentation:
    theme: united
    smaller: true 
css: custom.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, cache = TRUE)
```

## Source 

The R codes for the baseball example and the data are taken from the ISLR book by James, Witten, Hastie and Tibshirani. 

## Baseball data

Major League Baseball Data from the 1986 and 1987 seasons.

```{R, echo = T}
library(ISLR)
#fix(Hitters)
names(Hitters)
dim(Hitters)
```

## Variables {.smaller}

- `AtBat`: Number of times at bat in 1986, `Hits`: Number of hits in 1986
- `HmRun`: Number of home runs in 1986, `Runs`: Number of runs in 1986
- `RBI`: Number of runs batted in in 1986, `Walks`: Number of walks in 1986.
- `Years`: Number of years in the major leagues
- `CAtBat`: Number of times at bat during his career
- `CHits`: Number of hits during his career
- `CHmRun`: Number of home runs during his career
- `CRuns`: Number of runs during his career
- `CRBI`: Number of runs batted in during his career
- `CWalks`: Number of walks during his career
- `League`: A factor with levels A and N indicating player's league at the end of 1986
- `Division`: A factor with levels E and W indicating player's division at the end of 1986
- `PutOuts`: Number of put outs in 1986
- `Assists`: Number of assists in 1986
- `Errors`: Number of errors in 1986
- `Salary`: 1987 annual salary on opening day in thousands of dollars
- `NewLeague`: A factor with levels A and N indicating player's league at the beginning of 1987

## Remove NA's

`Salary` is missing for some players. The `na.omit` function
removes all of the rows that have missing values in any variable.

```{R, echo = T}
sum(is.na(Hitters$Salary))
Hitters=na.omit(Hitters)
dim(Hitters)
```

## Best Subset Selection

The `regsubsets()` function (part of the `leaps` library) performs best subset selection by identifying the best model that contains a given number of predictors, where best is quantified using RSS. 

```{r, echo = T}
library(leaps)
regfit.full=regsubsets(Salary~.,Hitters)
summary(regfit.full)
```



## Best Subset Selection

By default, `regsubsets()` reports up to the best eight-variable model, which we can change using the `nvmax` argument. 

```{R, echo = T}
regfit.full=regsubsets(Salary~.,data=Hitters,nvmax=19)
(reg.summary=summary(regfit.full))
```


## Best Subset Selection

- The `summary()` function also returns $R^2$, RSS, adjusted $R^2$, $C_p$, and BIC.
- We can examine these to try to select the best overall model.

```{r, echo = T}
#Performance measures
  cbind( 
    Cp     = summary(regfit.full)$cp,
    r2     = summary(regfit.full)$rsq,
    Adj_r2 = summary(regfit.full)$adjr2,
    BIC    =summary(regfit.full)$bic
)
```

- The $R^2$ values increase monotonically !

## Choosing the best model 

Plotting RSS, adjusted $R^2$, $C_p$, and BIC for all of the models at once will
help us decide which model to select:

```{r, echo = T, eval = F}
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
plot(regfit.full,scale="r2")
```


## Choosing the best model 

Plotting RSS, adjusted $R^2$, $C_p$, and BIC for all of the models at once will help us decide which model to select:

```{r, echo = F, eval = T}
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
plot(regfit.full,scale="r2")
```


## Choosing the best model 

The functions `which.max()` and `which.min()` helps us select the maximum point of a vector. 

```{r, echo = T, eval = T}
which.max(reg.summary$adjr2)
which.min(reg.summary$cp)
which.min(reg.summary$bic)
```


## Now plot everything together 

```{r, echo = F, eval = T}
par(mfrow=c(2,2))
plot(reg.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(reg.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
points(11,reg.summary$adjr2[11], col="red",cex=2,pch=20)
plot(reg.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
points(10,reg.summary$cp[10],col="red",cex=2,pch=20)
plot(reg.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
points(6,reg.summary$bic[6],col="red",cex=2,pch=20)
plot(regfit.full,scale="r2")
```

## Forward 

```{r, echo = T}
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="forward")
(fwd.summary <- summary(regfit.fwd))
```


## Plots

```{r, echo = F, eval = T}
par(mfrow=c(2,2))
plot(fwd.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(fwd.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
plot(fwd.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
plot(fwd.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
#plot(regfit.full,scale="r2")
```

## Best model 

```{r, echo = T, eval = T}
which.max(fwd.summary$adjr2)
which.min(fwd.summary$cp)
which.min(fwd.summary$bic)
```


## Now plot everything together 

```{r, echo = F, eval = T}
par(mfrow=c(2,2))
plot(fwd.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(fwd.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
points(11,fwd.summary$adjr2[11], col="red",cex=2,pch=20)
plot(fwd.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
points(10,fwd.summary$cp[10],col="red",cex=2,pch=20)
plot(fwd.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
points(6,fwd.summary$bic[6],col="red",cex=2,pch=20)
#plot(regfit.full,scale="r2")
```


## Backward 
```{r, echo = T}
regfit.bwd=regsubsets(Salary~.,data=Hitters,nvmax=19,method="backward")
bwd.summary <- summary(regfit.bwd)
```

## Best model 

```{r, echo = T, eval = T}
which.max(bwd.summary$adjr2)
which.min(bwd.summary$cp)
which.min(bwd.summary$bic)
```

## Now plot everything together 

```{r, echo = F, eval = T}
par(mfrow=c(2,2))
plot(bwd.summary$rss,xlab="Number of Variables",ylab="RSS",type="l")
plot(bwd.summary$adjr2,xlab="Number of Variables",ylab="Adjusted RSq",type="l")
points(11,bwd.summary$adjr2[11], col="red",cex=2,pch=20)
plot(bwd.summary$cp,xlab="Number of Variables",ylab="Cp",type='l')
points(10,bwd.summary$cp[10],col="red",cex=2,pch=20)
plot(bwd.summary$bic,xlab="Number of Variables",ylab="BIC",type='l')
points(8,fwd.summary$bic[8],col="red",cex=2,pch=20)
#plot(regfit.full,scale="r2")
```

## Different Models {.smaller}

```{r, echo = T}
coef(regfit.full,6)
coef(regfit.fwd,6)
coef(regfit.bwd,6)
```

## Predictions from regsubsets

```{r, echo = T}
##Function to get predictions from the regsubset function 
predict.regsubsets <- function(object, newdata, id,...){
  form  <- as.formula(object$call[[2]])
  mat   <- model.matrix(form, newdata)
  coefi <- coef(object, id=id)
  mat[, names(coefi)]%*%coefi
}
```

- We can use this function to extract predictions from any step of these models. 

- For example, we can check the predicted salary for the $i^{th}$ player by selecting the $i^{th}$ row.

```{r, echo = T}
i = sample(nrow(Hitters), size = 1)
cbind(i, predict.regsubsets(regfit.full, newdata = Hitters[i,], id = 6))
```


## LOOCV 

- Another option is to use cross-validation in this subset of model. Below, a leave-one-out cross-validation is used to choose the model with lowest MSE. 
- The function `regsubset()` does not include a prediction function. 


```{r, echo = TRUE}
##jack-knife validation (leave-one-out)

  #store the prediction error 
  jk.errors <- matrix(NA, nrow(Hitters), 19) 
  
  for (k in 1:nrow(Hitters)){
    #uses regsubsets in the data with 1 observation removed 
    best.model.cv <- regsubsets(Salary ~.,                        data = Hitters[-k,],          #removes the kth obsv
                        nvmax = 19) 

    #Models with 1, 2 ,...19 predictors
    for (i in 1:19){
      #that was left out
      pred <- predict(best.model.cv,                 #prediction in the obsv 
                      Hitters[k,], 
                      id=i)
      jk.errors[k,i] <- (Hitters$Salary[k]-pred)^2       #error in the obsv 
    }
  }
  
  mse.models <- apply(jk.errors, 2, mean)            #MSE estimation 
      plot(mse.models ,                              #Plot with MSEs
       pch=19, type="b",
       xlab="nr predictors",
       ylab="MSE")

```



# Regularized Regression 


## Ridge Regression 

-  Ridge, Lasso, Elastic Net are all based on R package `glmnet`.
- Elastic Net Penalty: 
$$
P_{\lambda,\alpha}(\beta) = \frac{(1-\alpha)}{2} {\lVert \beta \rVert}_2^2 + \alpha {\lVert \beta \rVert}_1^2
$$

- $\alpha = 0$ is Ridge, $\alpha = 1$ is Lasso, anything in between is Elastic Net. 

## R commands 

```{r, echo = T, warning=FALSE, message=FALSE}
x=model.matrix(Salary~.,Hitters)[,-1]
y=Hitters$Salary
dim(x)
dim(y)
library(glmnet)
grid=10^seq(10,-2,length=100)
ridge.mod=glmnet(x,y,alpha=0,lambda=grid)
dim(coef(ridge.mod))
```

## Solutions {.smaller}

When $\lambda$ = `r ridge.mod$lambda[50]` : 

```{r, echo = T}
ridge.mod$lambda[50]
coef(ridge.mod)[,50]
sqrt(sum(coef(ridge.mod)[-1,50]^2))
```

## Solutions {.smaller}

When $\lambda$ = `r ridge.mod$lambda[60]` : 

```{r, echo = T}
ridge.mod$lambda[60]
coef(ridge.mod)[,60]
sqrt(sum(coef(ridge.mod)[-1,60]^2))
```

## Plot 

```{r}
plot(ridge.mod,xvar="lambda",label=TRUE)
```
