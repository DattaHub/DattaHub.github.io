---
title: "Hint for HW4, Q4"
author: "Jyotishka Datta"
date: '2022-03-17'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

## Q4. Ridge regression (25 pts)

In this question we will study the so-called *ridge regression* estimator, and see how it is connected to lasso and principal components regressions.  Whereas lasso involves an "L1 penalty", ridge involves L2.

$$
\hat{\beta}^\mathrm{ridge}_\lambda = \mathrm{argmin}_\beta \; ||y - X \beta||^2 + \lambda \sum_{j=1}^p |\beta_j|^2.
$$

As in the lasso, it is typical to center the $x$-values so that the intercept can be inferred as $\hat{\beta}_0 = \bar{y}$ without penalty; then $y$ above would really be $y-\hat{\beta}_0$ and $X$ would not contain a column of ones.  So basically, assume no intercept.  (It is also common to normalize the $x$-values as described in class.)

One of the advantages of ridge over lasso is that the solution $\hat{\beta}$ is available in closed form.  Derive an expression for $\hat{\beta}^\mathrm{ridge}_\lambda$ that can be calculated with simple matrix-vector multiplications in the same way as in ordinary least squares.  

## Hint:

First, let us look at the case of multiple linear regression OLS estimate. OLS solves the following optimization:
$$
\hat{\beta}_{ols} =	\min_{\beta} \sum_{i=1}^{n} \left( y_i - \beta_0 - \sum_{j=1}^{p} \beta_j x_{i,j} \right)^2
$$
We can write this in matrix-vector notation as:

$$
\begin{align}
\hat{\beta}_{ols} & =	\min_{\beta} \left\{ (y-X\beta)^T(y-X\beta)\right\} \doteq \min_{\beta} \{ Q(\beta) \}
\end{align}
$$
Now we can differentiate the objective function $Q(\beta)$ with respect to $\beta$, set the (partial) derivatives to zero and get the solution for ridge regression. 

$$
\begin{align}
\frac{d}{d\beta} Q(\beta) & = \frac{d}{d\beta} \left\{ (y-X\beta)^T(y-X\beta) + \lambda \beta^T \beta \right\} \\
& = \frac{d}{d\beta} \{ y^Ty - 2 \beta^T X^T y + \beta^T X^T X \beta  \} \\
& = \frac{d}{d\beta} \{ y^Ty - 2 \beta^T X^T y + \beta^T (X^T X I) \beta \} \\
& = - 2 X^T y + 2 \beta (X^TX).
\end{align}
$$
Hence, $\frac{d}{d\beta} Q(\beta) = 0$ would lead to:

$$
\begin{align}
- 2 X^T y + 2 \beta (X^TX) & = 0 \\
\therefore, \quad \hat{\beta}_{ols} = (X^TX)^{-1}(X^Ty). 
\end{align}
$$


Observe that the penalized objective function for ridge regression  is identical to the following.

$$
(y - X\beta)^\top (y- X\beta) + \lambda \beta^\top \mathbb{I}_n \beta
= y^\top y - 2 \beta^\top X^\top y + \beta^\top(X^\top X + \lambda \mathbb{I}_n) \beta
$$

We could differentiate this and solve for $\beta$ to get the answer. 

