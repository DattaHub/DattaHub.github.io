install.packages("twitteR")
install.packages("tm")
install.packages("data.table")
install.packages("SnowballC")
install.packages("SentimentAnalysis")
library(SentimentAnalysis)
?SentimentAnalysis
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
library(data.table)
library(tm)
library(twitteR)
load(file="easports_tweets.Rdata") # Lazy loading
tweets.df[1, c("id", "created", "screenName", "replyToSN",
"favoriteCount", "retweetCount", "longitude",  "latitude", "text")]
writeLines(strwrap(tweets.df$text[1], 60))
## Text Cleaning
library(tm)
library(SnowballC)
# build a corpus, and specify the source to be character vectors
myCorpus <- Corpus(VectorSource(tweets.df$text))
# remove URLs
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeURL))
# remove anything other than English letters or space
removeNumPunct <- function(x) gsub("[^[:alpha:][:space:]]*", "", x)
myCorpus <- tm_map(myCorpus, content_transformer(removeNumPunct))
# remove stopwords
myStopwords <- c(setdiff(stopwords('english'), c("r", "big")),
"use", "see", "used", "via", "amp")
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
# remove extra whitespace
myCorpus <- tm_map(myCorpus, stripWhitespace)
# convert to lower case
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# keep a copy for stem completion later
myCorpusCopy <- myCorpus
myCorpus <- tm_map(myCorpus, stemDocument) # stem words
writeLines(strwrap(myCorpus[[190]]$content, 60))
replaceWord <- function(corpus, oldword, newword) {
tm_map(corpus, content_transformer(gsub),
pattern=oldword, replacement=newword)
}
myCorpus <- replaceWord(myCorpus, "players", "player")
myCorpus <- replaceWord(myCorpus, "games", "game")
tdm <- TermDocumentMatrix(myCorpus,control = list(wordLengths = c(1, Inf)))
tdm
save(tdm,file="easports_tdm.Rdata")
rm(list=ls()) # clean your workspace
load(file="easports_tdm.Rdata") # Lazy loading
load(file="easports_tweets.Rdata") # Lazy loading
library(tm)
# inspect frequent words
(freq.terms <- findFreqTerms(tdm, lowfreq = 200))
term.freq <- rowSums(as.matrix(tdm))
term.freq <- subset(term.freq, term.freq >= 200)
df <- data.frame(term = names(term.freq), freq = term.freq)
library(ggplot2)
ggplot(df, aes(x = reorder(term, -freq), y=freq)) + geom_bar(stat="identity") +
xlab("Terms") + ylab("Count") + coord_flip() +
theme(axis.text=element_text(size=10))
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# colors
library(RColorBrewer)
pal <- brewer.pal(9, "BuGn")[-(1:4)]
# plot word cloud
library(wordcloud)
install.packages("wordcloud")
m <- as.matrix(tdm)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
# colors
library(RColorBrewer)
pal <- brewer.pal(9, "BuGn")[-(1:4)]
# plot word cloud
library(wordcloud)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 3,
random.order = F, colors = pal)
# which words are associated with 'human'?
findAssocs(tdm, "game", 0.2)
# library(sentiment)
# sentiments <- sentiment(tweets.df$text)
# table(sentiments$polarity)
library(SentimentAnalysis)
sentiments <- analyzeSentiment(documents)
View(tweets.df)
# library(sentiment)
# sentiments <- sentiment(tweets.df$text)
# table(sentiments$polarity)
library(SentimentAnalysis)
sentiments <- analyzeSentiment(tweets.df$text)
table(convertToDirection(sentiments$SentimentQDAP))
# library(sentiment)
# sentiments <- sentiment(tweets.df$text)
# table(sentiments$polarity)
library(SentimentAnalysis)
sentiments <- analyzeSentiment(tweets.df$text)
sentiments$polarity <-convertToDirection(sentiments$SentimentQDAP)
table(sentiments$polarity)
# sentiment plot
sentiments$score <- 0
sentiments$score[sentiments$polarity == "positive"] <- 1
sentiments$score[sentiments$polarity == "negative"] <- -1
sentiments$date <- as.IDate(tweets.df$created)
result <- aggregate(score ~ date, data = sentiments, mean)
plot(result, type = "l", ylab= "Mean sentiment", xlab = "Date")
