---
title: "Wilson Disease Data Analysis"
author: "Jyotishka Datta <br> Department of Mathematical Sciences <br> University of Arkansas"
date: "September 7, 2019"
output: 
  html_document:
      toc: true
      number_sections: true
      toc_float: true
urlcolor: blue
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(results = 'asis',      # This is essential (can also be set at the chunk-level)
               comment = NA,
               prompt = FALSE,
               cache = TRUE,
               warning = FALSE,
               message = FALSE)
library(summarytools)
st_options(plain.ascii = FALSE,      
           style = "rmarkdown",      # This too
           footnote = NA,             # Avoids footnotes which would clutter the results
           subtitle.emphasis = FALSE  # This is a setting to experiment with - according to 
           )                                     # the theme used, it might improve the headings'
                                      # layout
```

```{r themes, echo=FALSE}
st_css()                              # This is a must; without it, expect odd layout,
```  

```{r packages, echo = FALSE}
library(pacman)
p_load(tidyverse, summarytools, ggplot2, knitr)
library("PerformanceAnalytics")
```


# Background

## Simple Descriptive Statistics 

First, we report the basic descriptive statistics for the numerical variables.  
Note that some of the covariates are ordinal in nature, they are not reported on Table 1.


```{r descriptives, echo = F}
rm(list = ls())
setwd("C:/Users/jd033/OneDrive/Documents/Jija/Wilson")

data = read.csv(file = "rawdata.csv", header = T, sep = ",")

library(tidyverse)

df = as.data.frame(data)

df = df %>% select(-SN, -Name, - Muttaion)

# names(df)

df = df %>% select(AgeAtDx, Sex, MMSE, CERU, MRI_HI, LFT, CBC, ComMutn,
                   COGNITION, DEPR, PSYCH, DYST, TRMR, CHOR, PARKIN, SPEECH, SWALLOW,
                   SALVN, GAIT, KFRING, SEIZURE, PYRSIGN, STEREO, FABScore)

df = df %>% mutate_at(c("MRI_HI", "DEPR", "PSYCH", "DYST", "TRMR", 
                        "CHOR", "PARKIN", "SPEECH", "SWALLOW",
                   "SALVN", "GAIT", "KFRING"),as.ordered)

df = df %>% mutate_at(c("COGNITION","SEIZURE","PYRSIGN","STEREO","ComMutn"), as.factor)

library(summarytools)
descr(df,stats = c("mean", "sd", "min", "max"), transpose = TRUE, style = "rmarkdown")
```

## Correlation 

Figure 1 shows the scatter plot as well as the correlation between the numerical variables. In particular, it shows: 

1.  The distributions for each variable. 
2.  Scatter plots with a fitted line.
3.  The value of correlation. 

```{r corrplots, echo = F, fig.height = 3.5, fig.cap= "Correlation Chart"}
dfnum = df %>% select(AgeAtDx, CERU, FABScore, MMSE)
chart.Correlation(dfnum, histogram=TRUE, pch=19)
```


## Association between factors 

Here we show the association between the factors in our data, measured by the Chi-square p-value as well as the Cramer's V. We split the association matrix into two halves for the sake of presentation, with 'Common Mutation' being the common variable on both the plots. The first figure suggests strong association between different clinical variables but not so much with Common Mutation, and the second figure suggests some association of Common Mutation with Parkinsons, Dystonia, Depression and Gait. 

```{r assoc_measures_1, echo = FALSE, fig.height = 3, fig.cap = "Association measures: chi-square and Cramer's V. Part 1"}
library(lsr)

# function to get chi square p value and Cramers V
f = function(x,y) {
  tbl = df %>% select(x,y) %>% table()
  chisq_pval = round(chisq.test(tbl)$p.value, 4)
  cramV = round(cramersV(tbl), 4) 
  data.frame(x, y, chisq_pval, cramV) }

# dford = df %>% select(-AgeAtDx, -CERU, -FABScore,
#                       -MMSE, -Sex, -LFT, - CHOR, - ComMutn, - MRI_HI)

dford = df %>% select(TRMR, SWALLOW, STEREO, SPEECH, SEIZURE, SALVN, ComMutn)

# create unique combinations of column names
# sorting will help getting a better plot (upper triangular)
df_comb = data.frame(t(combn(sort(names(dford)), 2)), stringsAsFactors = F)

# apply function to each variable combination
df_res = map2_df(df_comb$X1, df_comb$X2, f)

# plot results
df_res %>%
  ggplot(aes(x,y,fill=chisq_pval))+
  geom_tile()+
  geom_text(aes(x,y,label=cramV))+
  scale_fill_gradient(low="red", high="yellow")+
  theme_classic()
```


```{r assoc_measures_2, echo = FALSE, fig.height = 3, fig.cap = "Association measures: chi-square and Cramer's V. Part 2"}
dford2 = df %>% select(PYRSIGN, PSYCH, PARKIN, GAIT, DYST, DEPR, ComMutn)

# create unique combinations of column names
# sorting will help getting a better plot (upper triangular)
df_comb = data.frame(t(combn(sort(names(dford2)), 2)), stringsAsFactors = F)

# apply function to each variable combination
df_res = map2_df(df_comb$X1, df_comb$X2, f)

# plot results
df_res %>%
  ggplot(aes(x,y,fill=chisq_pval))+
  geom_tile()+
  geom_text(aes(x,y,label=cramV))+
  scale_fill_gradient(low="red", high="yellow")+
  theme_classic()
```


## Comparison across subgroups 

Figure 4-7 show how the four numerical variables viz. Age at diagnosis, Ceruloplasmin, MMSE (cognitive scale), and Fab scores. It seems from these box-plots that the differences for these variables between different levels of Common mutation are not significant, indicated by the substantial overlap of the probability regions. 


```{r boxplots_1, echo = F, fig.height = 3, fig.cap = "Boxplots for subgroup analysis: Age & Ceruloplasmin"}
cols <- rainbow(2, s = 0.5)
par(mfrow = c(1,2))
boxplot(AgeAtDx ~ ComMutn, data = df,
        col = cols,
        xaxs = FALSE)
legend("topright", inset=c(-0.5,0), fill = cols, legend = c(0,1), horiz = F)
boxplot(CERU ~ ComMutn, data = df,
        col = cols,
        xaxs = FALSE)
legend("topright", inset=c(-0.5,0), fill = cols, legend = c(0,1), horiz = F)
par(mfrow = c(1,1))
```

```{r boxplots_2, echo = F, fig.height = 3, fig.cap = "Boxplots for subgroup analysis: MMSE" }
par(mfrow = c(1,2))
boxplot(MMSE ~ ComMutn, data = df,
        col = cols,
        xaxs = FALSE)
legend("topright", inset=c(-0.5,0), fill = cols, legend = c(0,1), horiz = F)
boxplot(FABScore ~ ComMutn, data = df,
        col = cols,
        xaxs = FALSE)
legend("topright", inset=c(-0.5,0), fill = cols, legend = c(0,1), horiz = F)
par(mfrow = c(1,1))
```

# Methods 

## Penalized Logistic Regression

We want to fit a logistic regression model for explaining Common Mutation as a function of the available predictors. A logistic regression fits the log-odds of the probabilities as a linear function of the predictors, i.e. 

$$ 
\eta_i = \log(\frac{p_i}{1-p_i}) = \alpha + \sum_{j=1}^{p} x_{ij}\beta_j , \; i = 1, \ldots, n,
$$

where the $x_{ij}$ is the i-th observation for the j-th predictor. 

The usual regression approach would run into a problem here as we have a low-rank data. Here $n =  52$, but the number of predictors is more than the number of columns in our data frame, as each ordinal factor has to be recoded into a baseline category, and one predictor for each different level. 

The solution to this is to use a penalized regression such as the Elastic net. The elastic net regression minimizes the penalized log-likelihood 

$$
S = -l(y,\beta) + P_{\lambda,\alpha}(\beta),
$$

where $l(y,\beta)$ is the log-likelihood and $P_{\lambda,\alpha}(\beta)$ is the penalty term. For the logistic regression the log-likelihood is given by:

$$
l(y,\beta) = \sum_{i=1}^{n} y_i \log(p_i) +\sum_{i=1}^{n}(1-y_i)\log(1-p_i)
$$

and the Elastic net penalty term is given by:

$$
P_{\lambda,\alpha}(\beta) = \lambda \left \{ \frac{(1-\alpha)}{2} {\lVert \beta \rVert}_2^2 + \alpha {\lVert \beta \rVert}_1^2 \right \}
$$

We can use the same R package 'glmnet'. Change family = "gaussian" to family="binomial". 


```{r glmnet, echo = F, fig.height = 3, fig.cap = "Elastic Net with Binomial Likelihood"}
library(glmnetUtils)
cv.fit = cv.glmnet(ComMutn ~ AgeAtDx+ MMSE+ CERU + MRI_HI + COGNITION+ DEPR+ PSYCH+ DYST+
               TRMR+ CHOR+ PARKIN+ SPEECH+ SWALLOW +
                 SALVN+ GAIT+ KFRING+ SEIZURE+ PYRSIGN+
                 STEREO+ FABScore, family = "binomial", alpha = 0.5,
         data = df)
plot(cv.fit)
```

The plot shows the deviance (a goodness-of-fit statistic) as a function of the tuning parameter $\lambda$, and the number of variables in the model (top margin of the plot). *It shows that at the optimal value, Elastic net selects no variables at all, or in other words, fits the null model.*

Since our goal is to understand the relative importance of predictors in explaining common mutation, we can look at a smaller value of $\lambda$ (we take $\lambda = 0.1$ here) on the coefficient path, and see which predictors come out on the top. This will not be a powerful predictive model but can give us some insgights into their relationships. 


```{r coefplot, echo = F, fig.height = 3, fig.cap = "Coefficients for Penalized Logistic Regression"}
library(coefplot)
coefplot(cv.fit, lambda=0.1, sort='magnitude')
```

## Prediction 

Here we perform a supervised learning to predict common mutation, that is, divide the data into two equal halves - training and test, and then train the model on one half and perform prediction on the other. 

```{R, echo = F, results = 'asis'}
set.seed(1984)
df = df %>% filter(ComMutn != "NA")

train=sample(1:nrow(df), nrow(df)*0.5)
dftrain = df[train,]
dftest = df[-train,]

# df[!complete.cases(df),]

library(glmnetUtils)
cv.fit = cv.glmnet(ComMutn ~ AgeAtDx+ MRI_HI + COGNITION+ DEPR+ PSYCH+ DYST+
                     TRMR+ CHOR+ PARKIN+ SPEECH+ SWALLOW +
                     SALVN+ GAIT+ KFRING+ SEIZURE+ PYRSIGN+
                     STEREO+ FABScore, family = "binomial", alpha = 0.5,
                     data = dftrain)

glm.pred = predict(cv.fit, newdata = dftest, s = 0.1, type = "class")
row.pred = as.numeric(row.names(glm.pred))
y.test = dftest$ComMutn

# # library(knitr)
# cat("Misclassification Table \n")
# table(glm.pred, y.test)
# 
# 
# ftab = table(glm.pred, y.test)
# library(stargazer)
# stargazer(format(ftab, title = "Confusion Matrix", rownames = TRUE, quote=FALSE, justify="right"), type="html")
# 
misclass.prob = mean(glm.pred != y.test)
cat("Misclassification Probability", misclass.prob)

glm.prob = predict(cv.fit, newdata = dftest, s = 0.1, type = "response") %>% drop()
glm.pred = predict(cv.fit, newdata = dftest, s = 0.1, type = "class") %>% as.numeric() %>% drop()
# row.pred = as.numeric(row.names(glm.pred))
y.test = dftest$ComMutn %>% as.numeric()

error.data = rbind(data.frame(type="predicted probability", values = glm.prob),
                   data.frame(type = "predicted value", values = glm.pred))

error.data = cbind(error.data, x = 1:26)

library(ggplot2)
pred_plot <- ggplot(error.data, aes(y = values, group = type, x = x)) + 
  geom_point(aes(color = type, shape = type)) + theme_bw()

glm.prob = predict(cv.fit, newdata = dftest, s = 0.1, type = "response") %>% drop()
glm.pred = predict(cv.fit, newdata = dftest, s = 0.1, type = "class") %>% as.numeric() %>% drop()
# row.pred = as.numeric(row.names(glm.pred))
y.test = dftest$ComMutn %>% as.numeric()

error.data = rbind(data.frame(type = "predicted value", values = glm.pred),
                   data.frame(type = "truth", values = y.test-1))

error.data = cbind(error.data, x = 1:26)
misclass_plot <- ggplot(error.data, aes(y = values, group = type, x = x)) + 
  geom_point(aes(color = type, shape = type)) + theme_bw()

library(patchwork)
pred_plot / misclass_plot

```

The first panel on the figure shows the predicted probabilities and the actual prediction and the second shows the true labels vs the prediction. The misclassification probability is `r misclass.prob` (which is almost as bad as a random guess that has a misclassification probability of 0.5). From the second panel, it seems the prediction misses all the labels with true value "1", and vice versa, i.e. the predicted values with label "1" are incorrectly classified. This is not surprising since the elastic net model was not able to find any strongly correlated predictors. 









