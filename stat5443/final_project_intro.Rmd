---
title: "Project Ideas"
author: "Jyotishka Datta"
date: "April 9, 2018"
output: ioslides_presentation
fontsize: 10pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
```

<style>
  .col2 {
    columns: 2 200px;         /* number of columns and width in pixels*/
    -webkit-columns: 2 200px; /* chrome, safari */
    -moz-columns: 2 200px;    /* firefox */
  }
  .col3 {
    columns: 3 100px;
    -webkit-columns: 3 100px;
    -moz-columns: 3 100px;
  }
</style>

## Final Projects (Part 1 : Implementation)

1.  Compare and contrast penalized regression and Bayesian shrinkage priors for large $p$, small $n$ data. 
2.  Classification problem for large $p$, small $n$ data: compare interpretable methods (Lasso) and Random forests. 


## Final Projects (Part 2: Lit Review)

I'll point out a few interesting directions, but you can choose your own: 

1. Network Inference (Graph partition / Stochastic Block Models etc.) 
2. Efficient Inference for Bayesian Sparse Shrinkage Priors. 
3. Optimization and $\ell_0$ regularization. 
4. Topic Modeling - Latent Dirichlet Allocation. 

## Rules - Part 1

1.  Part 1 is a group project - students will form groups of 4 and submit to me (jd033@uark.edu) by 4/20/18. 
2.  Each team must do at least one of classification or regression and compare a few different methods. 
3.  Ideally you'd choose a different data-set than the one shown below, and choose 4 different methods to compare. 
4.  Your reports will be due by May 10th, 2018. 


## Rules - Part 2

1.  Part 2 is an individual project - each of the students will submit their own report. You can discuss papers you have chosen but do not review the same paper. 

2.  Each student must select at least one paper related to the topic and write at least a full page review but there is no upper bound.  

3.  Ideally you'd write a review that is clear enough for your classmates to understand, focus on the broader perspectives: e.g. Why is this method important? How does it improve over the state-of-the-art methods? What are the limitations? Do the authors provide any implementation (e.g. R package / MCMC steps)? Is there any theoretical justification? 

4.  Your literature review reports will be due by May 7th, 2018. 


## Community Detection Resources 

1. [Community detection and stochastic block models: recent developments](https://arxiv.org/pdf/1703.10146.pdf)
2. [Original paper: Stochastic Block Models](http://www.stat.cmu.edu/~brian/780/bibliography/04%20Blockmodels/Holland%20-%201983%20-%20Stochastic%20blockmodels,%20first%20steps.pdf)
3. [A. Clauset, M.E.J. Newman and C. Moore, - "Finding community structure in very large networks.", Physical Review E 70, 066111 (2004)](http://tuvalu.santafe.edu/~aaronc/courses/5352/fall2013/readings/Clauset_Newman_Moore_04_FindingCommunityStructureInVeryLargeNetworks.pdf).


## Efficient Bayesian Inference 

1.  [Fast sampling with Gaussian scale mixture priors in high-dimensional regression, Chakrabarty, Bhattacharya, Mallick](https://academic.oup.com/biomet/article/103/4/985/2447851)
2.  [GPU-accelerated Gibbs sampling, Terenin, Dong, Draper](https://arxiv.org/ftp/arxiv/papers/1608/1608.04329.pdf)
3.  [Comment: A brief survey of the current state of play for Bayesian computation in data science at big-data scale](https://arxiv.org/pdf/1712.00849.pdf)


## Optimization and $\ell_0$ regularization 

1.  [An Algorithmic Approach to Linear Regression - Bertsimas and King](https://pubsonline.informs.org/doi/pdf/10.1287/opre.2015.1436)
2.  [Best Subset Selection Via A Modern Optimization Lens, Bertsimas and Mazumder](https://projecteuclid.org/download/pdfview_1/euclid.aos/1458245736)
3.  [A Primal Dual Active Set Algorithm For A Class Of Nonconvex Sparsity Optimization](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.740.9614&rep=rep1&type=pdf)


## Topic Modeling - Latent Dirichlet Allocation 

1.  [Latent Dirichlet Allocation - Blei, Ng, Jordan](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
2.  [Introduction to Latent Dirichlet Allocation](http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/)
3.  [LDA Tutorial](http://obphio.us/pdfs/lda_tutorial.pdf)


## Other topics {.smaller}

There are many, other interesting topics to be explored. Here are some papers on diverse topics that I read over the last few weeks, and liked: 

1. [The xyz algorithm for fast interaction search in high-dimensional data](https://arxiv.org/pdf/1610.05108.pdf), Thanei, Meinhausen, Shah. 
2. [Mendelian Randomization with Poor Instruments: a Bayesian Approach](https://arxiv.org/pdf/1608.02990.pdf), Berzuini et al. 
3. [Finding Online Extremists in Social Networks](https://arxiv.org/pdf/1610.06242.pdf), Klausen et al. 
4. [Lasso, fractional norm and structured sparse estimation using a Hadamard product parametrization](https://arxiv.org/pdf/1611.00040.pdf), Peter Hoff. 
5. [A framework for probabilistic inferences from imperfect models](https://arxiv.org/pdf/1611.01241.pdf), Meng Li and David Dunson.
6. [Posterior Graph Selection and Estimation Consistency for High-dimensional Bayesian DAG Models](https://arxiv.org/pdf/1611.01205.pdf) by    Xuan Cao, Kshitij Khare, Malay Ghosh.


## Project 1: Data-sets

I'll show you two data-sets, but there are many more: 

1. Leukemia (Classification, Unsupervised Learning)
2. HIV (Regression, Supervised Learning).
3. This intro will show you how to read the two data-sets and get basic summary. It will also suggest some of the analysis that you can perform.
4. You can use one of the methods taught in class or try new methods - it's upto you (and your team). 
5. You need to present your work. 


## Source of High-Dimensional Datasets 

1.  [Data-sets from Patrick Breheny's course: High-Dimensional Data Analysis](http://myweb.uiowa.edu/pbreheny/7600/s16/data.html)
2.  [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)
3.  [Kaggle Open Data-sets](https://www.kaggle.com/datasets)



## Leukemia Data 

- Leukemia Data, Golub et al. Science $1999$.
- Matrix of gene expression levels for the 38 tumor mRNA samples, rows correspond to genes (3051 genes) and columns to mRNA samples.
- First install the `multtest` R package from `Bioconductor`.

```{r, echo = T, eval = F, cache=T}
source("https://bioconductor.org/biocLite.R")
biocLite("multtest")
```
```{r, echo = T, message=F, warning=F}
require(multtest)
data(golub)
dim(golub)
```

## Leukemia Data {.smaller}

Class labels:
```{r, echo = T}
golub.cl
```

Gene names:
```{r, echo = T}
dim(golub.gnames)
golub.gnames[1:4, ]
```

## Inferential Goals 

1. Visualize the data: e.g. compare the baseline expression values between two groups (ALL `golub.cl=0` vs AML `golub.cl=1`)
2. Perform unsupervised classification / clustering and calculate accuracy. 
3. Carry out Multiple testing. 
4. Identify Genes that are differentially expressed in two classes: Classification with variable selection.
5. Focus on new, efficient Methods e.g. K-NN, Logistic with $\ell_1$ penalty, Fisher's LDA or hierarchical clustering. 
6. K-NN has been covered, I will teach the other methods in class over the next weeks. 

## Supervised Learning for Leukemia Data

1. Try supervised classification with this data, the full data-set with $n = 72$ samples (38 training, 34 test) is available on the R bioconductor site. 

2. This site [https://github.com/ramhiser/datamicroarray/wiki/Golub-(1999)](https://github.com/ramhiser/datamicroarray/wiki/Golub-(1999)) has info regarding the full data-set and the paper. 


## HIV Data (Supervised)

PNAS paper studies in vitro drug resistance of $n = 1057$ HIV-1 isolates to protease and reverse transcriptase mutations. There are $p = 208$ (binary) mutation variables. 

The original paper compares 5 different regression methods: decision trees, neural networks, SVM regression, OLS and LASSO.

Full paper (if you're interested): [http://www.pnas.org/content/103/46/17355.full](http://www.pnas.org/content/103/46/17355.full)

## HIV Data

```{r, echo = T, tidy = T, cache=T}
setwd("C:/Users/Jyotishka Datta/OneDrive/Documents/Course Notes/stat5443/R codes/datasets")
## save the hiv.rda file somewhere that R can find and use that path
load("hiv.rda")
dim(hiv.train$x)
dim(hiv.test$x)
```

## Lasso {.smaller}

```{r, echo = T, warning = F, message = F, cache=T, fig.asp = 0.5}
require(glmnet)
set.seed(1)
cvfit <- cv.glmnet(hiv.train$x, hiv.train$y)
fit <- cvfit$glmnet.fit
plot(fit, xvar = "lambda")
```

## Restricted X-axis 

```{r, echo = T, warning = F, message = F, cache=T, fig.asp = 0.6}

xlim <- log(c(fit$lambda[1], cvfit$lambda.min))
plot(fit, xlim=xlim, xvar="lambda")
```

## Lasso {.smaller}

```{r, echo = T, warning = F, message = F, cache=T, fig.asp = 0.5}
plot(cvfit)
```


## MSE 

```{r, echo = T}
bestlam =cvfit$lambda.min
lasso.pred=predict(fit, s = bestlam , newx = hiv.test$x)
mean((lasso.pred - hiv.test$y)^2)
```

## Horseshoe 

- Another option in `monomovn` package is "hs" or horseshoe. 
- Horseshoe prior was proposed by Carvalho, Polson and Scott (2010) for sparse signal recovery. 
- Horseshoe prior improves on the Laplace prior used for Bayesian Lasso when the underlying data is nearly-black. 
- R package `horseshoe`, `monomvn`, `bayeslm` etc.


## Horseshoe R package {.smaller}

```{r, echo = T}
library(horseshoe)
hs.fit <- horseshoe(y = hiv.train$y, X = hiv.train$x, 
                    method.tau = "truncatedCauchy", method.sigma = "Jeffreys",
                    burn = 5000, nmc = 10000, thin = 2)
```

## Plot 

 Plot estimated coefficients: predicted values against the observed data
```{r, echo = T}
plot(hs.fit$BetaHat)
```


## Variable Selection {.smaller}

```{r, echo = T}
hs.pred <- hiv.test$x%*%hs.fit$BetaHat
(MSE = mean((hiv.test$y - hs.pred)^2))

hs.ind <- HS.var.select(hs.fit, hiv.train$y, method = "interval") 
cat("No of variables selected", sum(hs.ind), "\n")

cat("Estimates for selected coefficients \n", hs.fit$BetaHat[hs.ind], "\n")

cat("Which columns were selected?", which(hs.ind == 1))
```


## Credible Intervals {.smaller}

The credible intervals are really wide here - which suggests a poor uncertainty quantification and possibly poor fit. 

```{r, echo = T, message = F, warning= F}
library(Hmisc)
xYplot(Cbind(hs.fit$BetaHat, hs.fit$LeftCI, hs.fit$RightCI) ~ 1:length(hs.fit$BetaHat))
```



## Which one is correct? 

-  In absence of "ground truth", it is hard to tell if horseshoe is over-penalizing or Lasso is severely under-penalizing. 
-  That's why we choose methods based on a simulation study that we think accurately represents our reality (hard!)



## Inferential Goals 

1. Apply different sparse regression (regularization/shrinkage/dimension reduction technique) on this data. 

2. Compare between different methods? e.g. Would Elastic Net be a better fit than Lasso? Do Bayesian methods outperform frequentist methods? How do you test? How about Bayesian methods or PC Regression?

3. Could try extensions: The next slide show histogram of $y_{tr}$ - would it be better if we change the response to binary from continuous and apply classification methods? 

## Dichotomizing Response {.smaller}

<div style="float: left; width: 50%;">
```{r, echo = T, fig.width=3.5}
hist(hiv.train$y, col = "red")
abline(v = log(25,10))
```
</div>
<div style="float: right; width: 50%;">
```{r, echo = T}
ytr_b = hiv.train$y > log(25,10)
table(ytr_b)
```
Should we treat `ytr_b` as our new response and try a classification algorithm? 
</div>

## Using dichotomized response 

```{r, echo = T}
#library(glmnet)
fit.bin=glmnet(hiv.train$x,ytr_b, alpha = 1, family = "binomial")
plot(fit.bin)
```

## Side-by-side {.smaller}

```{r, echo = F}
par(mfrow=c(1,2))
plot(fit)
plot(fit.bin)
```

## Summary 

-  Response $y$ - discrete (0/1) or continuous. 
-  Predictors $X$ - $n \times p$, $p \gg n$. 
-  Compare different high-dimensiona methods. 
-  There are many data-sets (e.g. `brca` data for last lab), and we can also simulate data-sets where the ground truth is known. 


## Bioinformatic Analysis 

-  I will show a typical Bioinformatics analysis involving multiple testing. 
-  We observe an $n \times p$ matrix $X$.  
-  We know that the $n$ subjects belong to two different groups: disease/normal, type-a/type-b. 
-  I want to test which of the $p$ variables are different between the two groups? 

## Large Scale Testing

- We can carry out $p$ simultaneous tests for each of the variables: 
- Suppose for the $i^{th}$ variable $x_{i}$ the two group means are $\theta_{i,1}$ and $\theta_{i,2}$.
$$
H_{0i}: \theta_{i,1} = \theta_{i,2} \text{ vs. } H_{1i}: \theta_{i,1} \neq \theta_{i,2}
$$
- If $H_{0i}$ is true, the group means $\bar{x}_{i,1}$ and  $\bar{x}_{i,1}$ should be close. 
- We can do an independent samples t-test for each of the $p$ variables. - Need care about multiplicity !

## Example from T-cell lymphoma {.build}

```{r,echo=TRUE, message=FALSE, warning=FALSE, cache=T}
## required for gene expression data classification example
require(ALL)
data(ALL)
dim(ALL)
```

## Simplifying features 

We are going to use the first three features. 

```{r, echo=TRUE}
resp <- gsub("B[0-9]", "B", ALL$BT) 
## B-cell tumors of type B, B1,B2, T, T1, T2 
resp <- factor(gsub("T[0-9]", "T", resp))
xmat <- t(exprs(ALL))
mydata <- data.frame(y = resp,x1 = xmat[,1],x2=xmat[,2],x3=xmat[,3])
head(mydata,n=3)
```

## Further Analysis 

We can looks at the results of molecular biology testing for the 128 samples:

```{r, echo = T}
table(ALL$mol.biol)
```

## Filter 

Ignoring the samples which came back negative on this test (NEG), most have been classified as (BCR/ABL) or (ALL1/AF4).

For the purposes of this example, we are only interested in these two subgroups, so we will create a filtered version of the dataset using this as a selection criteria:

```{r, echo = T}
eset <- ALL[, ALL$mol.biol %in% c("BCR/ABL", "ALL1/AF4")]
dim(eset)
```

## Heatmap of the Genes? {.smaller}

The resulting variable, eset, contains just 47 samples - each with the full 12,625 gene expression levels. This is far too much data to draw a heatmap with, but we can do one for the first 100 genes as follows:

```{r, echo = T, cache=T}
heatmap(exprs(eset[1:100,]))
```

## How do we analyze this data?

- We have the expression levels 12,625 genes for 47 samples. 
- We also have a factor `ALL$mol.biol` that has two levels: BCR/ABL and ALL1/AF4. 
- We can ask for which genes, the gene expression values differ between these two subgroups? 

## Simple test 

- One idea would be use a two sample t-test for equality of group mean for each of the 12,625 genes. 
- The `mt.teststat` function from the `multtest` library does this for you. 

```{r, echo = T, cache=T}
require(multtest)
all.mat = exprs(eset)
all.cl <- factor(as.character(eset$mol.biol))
teststat = mt.teststat(all.mat,all.cl,test="t")
```

- mt.teststat {multtest}: **Computing test statistics for each row of a data frame**

- These functions provide a convenient way to compute test statistics, e.g., two-sample Welch t-statistics, Wilcoxon statistics, F-statistics, paired t-statistics, block F-statistics, for each row of a data frame.

## Visualize 

```{r, echo = F}
par(mfrow=c(1,2))
qqnorm(teststat, ylim = c(-5,5))
qqline(teststat)
hist(teststat, breaks = 30, col = "red", freq = F, xlim = c(-5,5), ylim = c(0,0.5))
curve(dnorm(x), lty = 2, lwd = 2, add=TRUE)
curve(dnorm(x, mean = mean(teststat), sd = sd(teststat)), lty = 3, lwd = 2, add=TRUE)
```

- Histogram and N(0,1) density different on the tails - a few interesting genes?


## Multiple Testing Issues 

- We have a large number of tests: 12,625. If we use standard hypothesis testing at a 5% significance level, 5% of all tests will be falsely rejected (type 1 error) just by pure chance. 

- We need some kind of multiplicity control. 

- The most stringent is Bonferroni: Divide each $\alpha$ by the total number of tests $p = 12,625$. 

## Bonferroni 

- Bonferroni's correction controls for the familywise error rate (FWER) instead of each $\alpha$. 
$$
FWER = P(\text{at least one false rejection}) \le \alpha 
$$

- Bonferroni leads to a stringent test, since $\alpha/p$ could be very small if we are carrying out a large number of $p$ tests simultaneously. 
- In R, we can apply the `p.adjust` function for this task. 

- `p.adjust` also has other useful methods such as "Benjamini-Hochberg False Discovery Rate control procedure".

## Bonferroni 

```{r, echo = T}
rawp = 2 * (1 - pnorm(abs(teststat)))
selected  <- p.adjust(rawp, method = "bonferroni") <0.05
esetSel <- eset [selected, ]
sum(selected)
```

- Bonferroni's correction leads to rejection of `r sum(selected)
` tests - these genes significantly differ between two groups

## Visualize the outcomes

```{r, echo = T}
heatmap(exprs(esetSel))
```

## Better heatmap {.smaller}

```{r, echo = T}
library(RColorBrewer)
hmcol<-brewer.pal(11,"RdBu")
heatmap(exprs(esetSel), col = hmcol, xlab = "Significant Genes", ylab =  "Samples")
```

## Bonferroni 

- $M$ hypothesis tests: $H_{0m}$ vs. $H_{1m}$ for $m = 1, \ldots, M$. 
- Let $p_1, \ldots, p_M$ be the p-values for these $M$ tests. 
- In our case $M = p$ (no. of genes)
- Bonferroni method: 
$$
\text{Reject null hypothesis } H_{0m} \text{ if } p_m \le \frac{\alpha}{M}
$$

- Outcome: The probability of falsely rejecting any null hypothesis is less than or equal to $\alpha$. 

## Benjamini-Hochberg {.smaller}

- Let $M_0$ be the number of null hypotheses that are true, $M_0 = M - M_1$. 

|           |$H_0$ acc |	$H_0$ rej | Total |
|-----------|----------|------------|-------|
|$H_0$ true |	U        | V          | $M_0$ |
|$H_0$ false|	T        | S          | $M_1$ |
|Total      |	M-R      | R          | $M$   |

Define the false discovery proportion (FDP): 
$$
FDP = \begin{cases} 
V/R \text{ if } R > 0 \\
0 \text { otherwise}
\end{cases}
$$

## Benjamini-Hochberg {.smaller}

- $M$ hypothesis tests We order the p-values in increasing order.
$p_{(1)} \le \ldots \le p_{(M)}$. 
- *Benjamini-Hochberg Method* 
    1.  For a given $\alpha$ find the largest $k$ such that
$$
p_{(k)} \le k \frac{\alpha}{M}
$$
    2.  Then reject all $H_{0m}$ for $m = 1, \ldots, k$.
-  *Theorem* : 
$$
FDR = E(FDP) \le \frac{M_0}{M}\alpha \le \alpha
$$
-  *Outcome*: For a given significance level $\alpha$, the Benjamini Hochberg method bounds the false discovery rate.

## Benjamini-Hochberg

```{r, echo = T}
rawp = 2 * (1 - pnorm(abs(teststat)))
selected  <- p.adjust(rawp, method = "BH") <0.05
esetSel <- eset [selected, ]
sum(selected)
```

- Benjamini-Hochberg method leads to rejection of `r sum(selected)
` tests - less stringent than Bonferroni. 

## Benjamini-Hochberg's heatmap {.smaller}

```{r, echo = T}
library(RColorBrewer)
hmcol<-brewer.pal(11,"RdBu")
heatmap(exprs(esetSel), col = hmcol, xlab = "Significant Genes", ylab =  "Samples")
```

## Many different ways

- There are many different ways of doing the same analysis - we usually pick the one that gives us a reasonably accurate and yet meaningful answer. 

- For example, in a Statistical genetics study, your collaborator (a geneticist) might have some prior biological knowledge about which genes should be different, and that provides a way of checking validity. 

- I will show another multiple testing function without explaining the stats behind it - the point is to show that outcomes could differ. 

## Empirical Bayes 

There's an `lmFit` function (from the `limma` package) to look for genes differentially expressed between the two groups. 

The fitted model object is further processed by the `eBayes` function to produce empirical Bayes test statistics for each gene. 

```{r, echo = T, warning = F, message = F}
library(limma)
f <- factor(as.character(eset$mol.biol))
design <- model.matrix(~f)
fit <- eBayes(lmFit(eset,design))
```

## P-values an all {.smaller}

```{r, echo = T}
topTable(fit, coef=2)
```

The leftmost numbers are row indices, ID is the Affymetrix HGU95av2 accession number, M is the log ratio of expression, A is the log average expression, t the moderated t-statistic, and B is the log odds of differential expression.

## Selection 

Next, we select those genes that have adjusted p-values below 0.05, using a Benjamini-Hochberg method to select a small number of genes.

```{r, echo = T}
selected  <- p.adjust(fit$p.value[, 2], method = "BH") <0.05
esetSel <- eset [selected, ]
sum(selected)
```

## Heatmap 

The variable esetSel has data on (only) 749 genes for all 47 samples . We can easily produce a heatmap as follows:

```{r, echo = T}
heatmap(exprs(esetSel))
```

## Selection 

We could be very stringent - use Bonferroni! 

```{r, echo = T}
selected  <- p.adjust(fit$p.value[, 2], method = "bonferroni") <0.05
esetSel <- eset [selected, ]
sum(selected)
```

## Heatmap 

The variable `esetSel` has data on (only) 165 genes for all 47 samples . We can easily produce a heatmap as follows:

```{r, echo = T}
heatmap(exprs(esetSel))
```


