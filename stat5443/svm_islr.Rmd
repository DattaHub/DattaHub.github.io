---
title: "SVM R Demo"
author: "Jyotishka Datta"
date: "April 26, 2017"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
knitr::opts_chunk$set(cache = TRUE)
```

## SVM 

 - We use the R package `e1071` for fitting an SVM to the data.
 - There are other versions as SVM is very popular. You should also try to implement SVM on your own. 

## SVM 

 - The `svm` function requires a few parameters apart from model and data.
 - Requires a `kernel`: could be any of `linear`, `polynomial`,`radial`.
 - Requires `cost`: Different from budget. 
 - `cost`: cost of a violation to the margin. When the cost argument is small, then the margins will be wide and many support vectors will be on the margin or will violate the margin. When the cost argument is large, then the margins will be narrow and there will be few support vectors on the margin or violating the margin.
 
## Generate Data 

 - Generate non-linear boundary to emphasize SVM. 

```{r}
library(e1071)
set.seed(1)
x=matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
```

## Plot the data 

```{r}
plot(x, col=y)
```

## Now fit SVM 

 - We use a radial kernel: 
$$
K(i,k) = \exp(-\gamma \lVert x_i - x_k \rVert^2)
$$
 - The `gamma` argument can take value for the $\gamma$ parameter. 
```{r}
train=sample(200,100)
svmfit=svm(y~., data=dat[train,], kernel="radial",  gamma=1, cost=1)
```
 - If you fit a polynomial kenel, you can specify the `degree` parameter. 
 

## Plot the SVM boundary 

```{r}
plot(svmfit, dat[train,])
```

## Summary of SVM {.smaller}

```{r}
summary(svmfit)
```

## Higher value cost 

```{r}
svmfit=svm(y~., data=dat[train,], kernel="radial",gamma=1,cost=1e5)
plot(svmfit,dat[train,])
```

## Effect of Cost 

 -  If we increase the value of cost, we can reduce the number
of training errors. 
 -  However, this comes at the price of a more irregular
decision boundary that seems to be at risk of overfitting the data.

## How to select the best parameters?

 - The e1071 library includes a built-in function, `tune()`, to perform cross-validation. 
 -  By default, `tune()` performs ten-fold cross-validation on a set
of models of interest. 
 -  In order to use this function, we pass in relevant
information about the set of models that are under consideration. 

## Best choice of $\gamma$ and cost {.smaller}

```{r}
set.seed(1)
tune.out=tune(svm, y~., data=dat[train,], kernel="radial", ranges=list(cost=c(0.1,1,10,100,1000),gamma=c(0.5,1,2,3,4)))
summary(tune.out)
```

## Predicted Class Labels

```{r}
table(true=dat[-train,"y"], pred=predict(tune.out$best.model,
                                         newdata=dat[-train,]))
```

  - 10% of the test observations are misclassified. 
  
## Multiple Classes 

 - If the response is a factor containing more than two levels, then the `svm()` function will perform multi-class classification using the one-versus-one approach.
 - Generate a third class of observations. 
 
```{r}
set.seed(1)
x=rbind(x, matrix(rnorm(50*2), ncol=2))
y=c(y, rep(0,50))
x[y==0,2]=x[y==0,2]+2
dat=data.frame(x=x, y=as.factor(y))
```


## Three classes 

```{r}
par(mfrow=c(1,1))
plot(x,col=(y+1))
```


## SVM 

```{r}
svmfit=svm(y~., data=dat, kernel="radial", cost=10, gamma=1)
plot(svmfit, dat)
```


## Gene Expression Data 

We now examine the Khan data set, which consists of a number of tissue
samples corresponding to four distinct types of small round blue cell tumors.

```{r}
library(ISLR)
names(Khan)
dim(Khan$xtrain)
dim(Khan$xtest)
```

## Class proportions 

```{r}
table(Khan$ytrain)
table(Khan$ytest)
```

## SVM 

  - We will use a support vector approach to predict cancer subtype using   gene expression measurements. 
  -  This is a wide data set: $p \gg n$. 
  -  Here Linear kernel might actually work well, because the additional flexibility that will result from using a polynomial or radial kernel is unnecessary.
  - For $n > p$, as in the example before, the reverse is true. 
  
## SVM fit on training data 

```{r}
dat=data.frame(x=Khan$xtrain, y=as.factor(Khan$ytrain))
out=svm(y~., data=dat, kernel="linear",cost=10)
#summary(out)
table(out$fitted, dat$y)
```

 - 0 errors but this is training data. 


## SVM fit on test data 

```{r}
dat.te=data.frame(x=Khan$xtest, y=as.factor(Khan$ytest))
pred.te=predict(out, newdata=dat.te)
table(pred.te, dat.te$y)
```

  - Just two test set errors. 
  
