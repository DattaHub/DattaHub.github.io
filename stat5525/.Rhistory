var.imp.plot.xgb1 <-ggplot(data = importance_matrix1, aes(x=reorder(Feature,Gain), y=Gain, fill="#BD0026")) +
geom_bar(stat = "identity", colour = "black", size =1) +
scale_fill_manual (values = "#BD0026") +
labs(title = "XG Boost: Variable Importance",
x="Variable",
y="Variable Importance") +
theme(legend.position = "none")
var.imp.plot.xgb1 + coord_flip()
var.imp.plot.xgb1 + coord_flip() + theme_minimal()
var.imp.plot.xgb1 <-ggplot(data = importance_matrix1, aes(x=reorder(Feature,Gain), y=Gain, fill="#BD0026")) +
geom_bar(stat = "identity", colour = "black", size =1) +
labs(title = "XG Boost: Variable Importance",
x="Variable",
y="Variable Importance") +
theme(legend.position = "none")
var.imp.plot.xgb1 + coord_flip() + theme_minimal()
var.imp.plot.xgb1 <-ggplot(data = importance_matrix1, aes(x=reorder(Feature,Gain), y=Gain)) +
geom_bar(stat = "identity", colour = "black", size =1) +
scale_fill_manual(values = "#BD0026") +
labs(title = "XG Boost: Variable Importance",
x="Variable",
y="Variable Importance") +
theme(legend.position = "none")
var.imp.plot.xgb1 + coord_flip() + theme_minimal()
var.imp.plot.xgb1 <-ggplot(data = importance_matrix1, aes(x=reorder(Feature,Gain), y=Gain)) +
geom_bar(stat = "identity",size =1) +
scale_fill_manual(values = "#BD0026") +
labs(title = "XG Boost: Variable Importance",
x="Variable",
y="Variable Importance") +
theme(legend.position = "none")
var.imp.plot.xgb1 + coord_flip() + theme_minimal()
lasso.coef <- predict(cv.out,type ="coefficient",s=bestlam)
plot(lasso.coef,pch=15,col="red", ylab = expression(beta), main = "Estimates")
points(beta,pch=15,col="blue")
plot(beta,pch=15,col="red", ylab = expression(beta), main = "Estimates")
points(lasso.coef,pch=15,col="blue")
plot(beta,pch=15,col="red", ylab = expression(beta), main = "Estimates vs true coef")
points(lasso.coef,pch=15,col="blue")
y=rbinom(n, size = 1, prob= logit.inv(fx))
train=sample(200,100) ## for supervised learning
library(glmnet)
## Cross validation to learn tuning parameter
cv.out <- cv.glmnet(x = x, y = y, alpha = 1, subset = train, family = "binomial") # binomial for binary response
plot(cv.out)
title("Cross Validation",line=2.5)
(bestlam =cv.out$lambda.min)
y.pred=predict(cv.out,type ="class",s=bestlam, newx = x[-train,])
y.obs <- y[-train]
xtabs(~ y.obs +y.pred) ## Misclassification table.
lasso.coef <- predict(cv.out,type ="coefficient",s=bestlam)
plot(beta,pch=15,col="red", ylab = expression(beta), main = "Estimates vs true coef")
points(lasso.coef,pch=15,col="blue")
library(pacman)
p_load(xgboost) ## or install then library
y_train = y[train]
x_train = x[train,]
x_test = x[-train,]
y=rbinom(n, size = 1, prob= logit.inv(fx+eps))
train=sample(200,100) ## for supervised learning
library(glmnet)
## Cross validation to learn tuning parameter
cv.out <- cv.glmnet(x = x, y = y, alpha = 1, subset = train, family = "binomial") # binomial for binary response
plot(cv.out)
title("Cross Validation",line=2.5)
(bestlam =cv.out$lambda.min)
y.pred=predict(cv.out,type ="class",s=bestlam, newx = x[-train,])
y.obs <- y[-train]
xtabs(~ y.obs +y.pred) ## Misclassification table.
lasso.coef <- predict(cv.out,type ="coefficient",s=bestlam)
plot(beta,pch=15,col="red", ylab = expression(beta), main = "Estimates vs true coef")
points(lasso.coef,pch=15,col="blue")
library(pacman)
p_load(xgboost) ## or install then library
y_train = y[train]
x_train = x[train,]
x_test = x[-train,]
#run XG Boost
fit_xgb <- xgboost(data.matrix(x[train,]), y_train
, max_depth = 7
, eta = 0.02
, nthread = 2
, nrounds = 3000
, subsample = .7
, colsample_bytree = .7
, booster = "gbtree"
, eval_metric = "rmse"
, objective="binary:logistic")
#generating predctions
pred.xgb <- predict(fit_xgb, data.matrix(x_test))
ypred.xgb = (pred.xgb>0.5)
xtabs(~ y.obs +ypred.xgb) ## Misclassification table.
#show variable importance
importance_matrix1 <- xgb.importance(colnames(x_train), model = fit_xgb)
library(ggplot2)
var.imp.plot.xgb1 <-ggplot(data = importance_matrix1, aes(x=reorder(Feature,Gain), y=Gain)) +
geom_bar(stat = "identity",size =1) +
scale_fill_manual(values = "#BD0026") +
labs(title = "XG Boost: Variable Importance",
x="Variable",
y="Variable Importance") +
theme(legend.position = "none")
var.imp.plot.xgb1 + coord_flip() + theme_minimal()
table(y)
x=matrix(rnorm(n*p),n,p)
x[1:180,]=x[1:180,]+2
eps=rnorm(n,mean=0,sd=0.1)
fx = x %*% beta
y=rbinom(n, size = 1, prob= logit.inv(fx+eps))
table(y)
x[1:150,]=x[1:150,]+2
## Generate data
n=200;p=50
p1 = 5
beta <- c(rep(3,p1),rep(0,p-p1)) ## First p1 beta non-zero, remaining p-p1 zeros.
x=matrix(rnorm(n*p),n,p)
x[1:150,]=x[1:150,]+2
eps=rnorm(n,mean=0,sd=0.1)
fx = x %*% beta
y=rbinom(n, size = 1, prob= logit.inv(fx+eps))
table(y)
## Generate data
n=200;p=50
p1 = 5
beta <- c(rep(3,p1),rep(0,p-p1)) ## First p1 beta non-zero, remaining p-p1 zeros.
x=matrix(rnorm(n*p),n,p)
x[1:100,]=x[1:100,]+2 ## Perturb first 100
eps=rnorm(n,mean=0,sd=0.1)
fx = x %*% beta
y=rbinom(n, size = 1, prob= logit.inv(fx+eps))
table(y)
table(y)
train=sample(200,100) ## for supervised learning
library(glmnet)
## Cross validation to learn tuning parameter
cv.out <- cv.glmnet(x = x, y = y, alpha = 1, subset = train, family = "binomial") # binomial for binary response
plot(cv.out)
title("Cross Validation",line=2.5)
(bestlam =cv.out$lambda.min)
y.pred=predict(cv.out,type ="class",s=bestlam, newx = x[-train,])
y.obs <- y[-train]
xtabs(~ y.obs +y.pred) ## Misclassification table.
lasso.coef <- predict(cv.out,type ="coefficient",s=bestlam)
plot(beta,pch=15,col="red", ylab = expression(beta), main = "Estimates vs true coef")
points(lasso.coef,pch=15,col="blue")
library(pacman)
p_load(xgboost) ## or install then library
y_train = y[train]
x_train = x[train,]
x_test = x[-train,]
#run XG Boost
fit_xgb <- xgboost(data.matrix(x[train,]), y_train
, max_depth = 7
, eta = 0.02
, nthread = 2
, nrounds = 3000
, subsample = .7
, colsample_bytree = .7
, booster = "gbtree"
, eval_metric = "rmse"
, objective="binary:logistic")
#generating predctions
pred.xgb <- predict(fit_xgb, data.matrix(x_test))
ypred.xgb = (pred.xgb>0.5)
xtabs(~ y.obs +ypred.xgb) ## Misclassification table.
#show variable importance
importance_matrix1 <- xgb.importance(colnames(x_train), model = fit_xgb)
library(ggplot2)
var.imp.plot.xgb1 <-ggplot(data = importance_matrix1, aes(x=reorder(Feature,Gain), y=Gain)) +
geom_bar(stat = "identity",size =1) +
scale_fill_manual(values = "#BD0026") +
labs(title = "XG Boost: Variable Importance",
x="Variable",
y="Variable Importance") +
theme(legend.position = "none")
var.imp.plot.xgb1 + coord_flip() + theme_minimal()
mat = matrix(rnorm(218*3),nrow=218)
kings<- mat[1:19,3] + 3
Robins<-mat[20:38,3]
Blues<-mat[40:58,3]
Waxs<-mat[60:78,3]
Bobos<-mat[80:98,3]
Cows<- mat[100:118,3]
Bunts<-mat[120:138,3]
Golds<-mat[140:158,3]
Savas<-mat[160:178,3]
Vesps<-mat[180:198,3]
Chips<-mat[200:218,3]
stem<- data.frame(kings,Robins,Blues,Waxs,Bobos,Cows,Bunts,Golds,Savas,Vesps,Chips )
#rownames(stem) <- letters[1:nrow(stem)]
boxplot(stem)
stamp<-stack(stem)
names(stamp)<-c("length","stem")
a1<-aov(length~stem,data=stamp)
summary(a1)
tk<-TukeyHSD(a1)
tk
plot(tk)
birds.fit<-aov(length~stem,data=stamp)
summary(birds.fit)
library(multcomp)
birds.mc <- glht(birds.fit, linfct = mcp(stem = "Tukey"))
install.packages("multcomp")
library(multcomp)
birds.mc <- glht(birds.fit, linfct = mcp(stem = "Tukey"))
birds.ci <- confint(birds.mc, level = 0.95)
birds.ci
plot(birds.ci, main = "", xlab = "stem length", cex.axis = 0.5)
row.names(birds.lmat) <- levels(stamp$ind)
library("HH")
t(birds.mc$linfct)
plot(birds.mmc2, x.offset = 8, main = "", main2 = "", cex.axis = 0.5)
birds.lmat <- cbind("Robins-kings" = c(-1, 1, rep(0,9)),
"Blues-kings" = c(-1,0, 1, rep(0,8)),
"Vesps-Chips" = c(rep(0,9),1,-1))
birds.mmc2 <- mmc(birds.fit, linfct = mcp(stem = "Tukey"),
focus.lmat = birds.lmat)
plotMatchMMC(birds.mmc2$mca, main = "", cex.axis = 0.5,
xlim = c(-5,10))
stem<- data.frame(kings,Robins,Blues,Waxs,Bobos,Cows,Bunts,Golds,Savas,Vesps,Chips )
rownames(stem) <- letters[1:nrow(stem)]
boxplot(stem)
stamp<-stack(stem)
names(stamp)<-c("length","stem")
a1<-aov(length~stem,data=stamp)
summary(a1)
tk<-TukeyHSD(a1)
tk
plot(tk)
cov_ind_districts = read.csv(url("https://api.covid19india.org/csv/latest/districts.csv"))
names(cov_ind_districts)
library(dplyr)
levels(cov_ind_districts$State)
table(cov_ind_districts$State)
3states_dist <- cov_ind_districts %>%
filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu"))
library(dplyr)
3states_dist <- cov_ind_districts %>% filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu"))
library(dplyr)
3states_dist <- cov_ind_districts %>% filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu"))
3states_dist = cov_ind_districts %>% filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu"))
str(cov_ind_districts)
?filter
3states_dist = cov_ind_districts %>% dplyr::filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu"))
3states_dist = cov_ind_districts %>% dplyr::filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu"))
3states_dist = cov_ind_districts %>% dplyr::filter(State == "Maharashtra")
cats <- read.csv("data/herding-cats-small.csv")
setwd("~/GitHub/DattaHub.github.io/stat5525")
cats <- read.csv("data/herding-cats-small.csv")
cats[, "coat"]  ## usual way
select(cats, coat)
select(cats, sex)
select(cats, coat, cat_id, fixed) #maintains order
cats$coat == "black"
cats[cats$coat == "black", ] ## usual way
filter(cats, coat == "black")
arrange(cats, coat)
# you can include additional columns to help sort the data
arrange(cats, coat, sex)
mutate(cats, weight = round(weight, 2))
mutate(cats, new_variable = age + weight)
mutate(cats, new_var_1 = age + weight, new_var_2 = age * weight)
# for example
sort(round(sqrt(cats$age * 2), 3))
(cats$age * 2) %>%
sqrt() %>%
round(3) %>%
sort()
## ----Another example --------------------------------------------------------------------
round(1.23456789, 3)
## ----Using pipe --------------------------------------------------------------------
1.23456789 %>% round(3)
3 %>% round(1.23456789, .)
select(filter(cats, coat == "black"), cat_id)
filter(cats, coat == "black") %>% select(cat_id)
names(cov_ind_districts)
3states_dist = cov_ind_districts %>%
dplyr::filter(State == "Maharashtra")
str(cov_ind_districts
)
cov_ind_districts %>%
dplyr::filter(State == "Maharashtra")
dat <- cov_ind_districts %>% dplyr::filter(State == "Maharashtra")
table(dat$State)
dat <- cov_ind_districts %>% dplyr::filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu")
)
table(dat$State)
str(dat)
library(xlsx)
write.xlsx(dat, paste0("districtdata3states",Sys.Date(),".xlsx"))
library(rio)
write.xlsx(dat, paste0("districtdata3states",Sys.Date(),".xlsx"))
install.packages("xlsx")
library(xlsx)
write.xlsx(dat, paste0("districtdata3states",Sys.Date(),".xlsx"))
library(xlsx)
write.xlsx(dat, paste0("districtdata3states",Sys.Date(),".xlsx"))
library(readxl)
library(pacman)
p_load(RCurl, tidyverse, glue, DT, plyr, plotly, RColorBrewer)
p_load(janitor, tidyr, dplyr)
library(readxl)
write.xlsx(dat, paste0("districtdata3states",Sys.Date(),".xlsx"))
write.xlsx2(dat, paste0("districtdata3states",Sys.Date(),".xlsx"),sheetName="Sheet1",
col.names=TRUE, row.names=TRUE, append=FALSE)
install.packages("xlsx")
library(xlsx)
write.csv(dat, paste0("districtdata3states",Sys.Date(),".csv"))
getwd()
x = 3
y = 2
if ((x %% 2 != 0) & (y %% 2 == 0)) {
stop("Condition error occurred")
} else if (x > 3 & y <= 3) {
"Hello world!"
} else if (x > 3) {
"!dlrow olleH"
} else if (x <= 3) {
"Something else..."
}
x = 2
y = 2
if ((x %% 2 != 0) & (y %% 2 == 0)) {
stop("Condition error occurred")
} else if (x > 3 & y <= 3) {
"Hello world!"
} else if (x > 3) {
"!dlrow olleH"
} else if (x <= 3) {
"Something else..."
}
primes <- c(2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97)
x <- c(3, 4, 12, 19, 23, 48, 50, 61, 63, 78)
for (i in x) {
if (any(i == primes))
next
print(i)
}
ggplot(cats, aes(x = roamer, y = wander_dist)) +
geom_boxplot(width = 0.5)
library(dplyr)
library(ggplot2)
cats <- read.csv('data/herding-cats.csv') %>% tbl_df
ggplot(cats, aes(x = weight, y = wander_dist)) +
geom_point() +
geom_smooth(method = 'lm', formula = y ~ x, se = FALSE, size = 3) +
theme_bw(base_size = 18)
weight_fit <- lm(formula = wander_dist ~ weight, data = cats)
summary(weight_fit)
wander_fit <- lm(formula = wander_dist ~ weight + age , data = cats)
summary(wander_fit)
wander_fit <- lm(formula = wander_dist ~ weight + age + fixed, data = cats)
summary(wander_fit)
```
wander_fit <- lm(formula = wander_dist ~ weight + age + factor(coat) + sex, data = cats)
summary_fit <- summary(wander_fit)
summary_fit$coefficients
wander_fit <- lm(formula = wander_dist ~ weight * age, data = cats)
summary(wander_fit)
ggplot(cats, aes(x = roamer, y = wander_dist)) +
geom_boxplot(width = 0.5)
roamer_fit <- glm(formula = roamer ~ wander_dist, data = cats, family = binomial )
cats <- cats %>% mutate(roamer = as.factor(roamer))
roamer_fit <- glm(formula = roamer ~ wander_dist, data = cats, family = binomial )
glm_summary <- summary(roamer_fit)
glm_summary
roamer_fit <- glm(formula = roamer ~ wander_dist + weight, data = cats, family = binomial )
summary(roamer_fit)
```
cats <- cats %>% mutate(roamer = as.factor(roamer))
ggplot(cats, aes(x = weight, y = wander_dist, color = roamer)) +   geom_point(size = 2) +
geom_smooth(method = 'lm', se = FALSE, size = 3) +
theme_bw(base_size = 18)
# cats$roamer <- relevel(cats$roamer, ref = 'yes')
roamer_fit <- glm(formula = roamer ~ wander_dist + weight, data = cats, family = binomial )
glm_summary <- summary(roamer_fit)
glm_summary
names(glm_summary)
glm_summary$coefficients
glm_summary$null.deviance
glm_summary$deviance
glm_summary$aic
new_cats <- data.frame(wander_dist = c(0.15, 0.10, 0.05),
weight = c(2, 6, 12))
new_cats
predicted_logit <- predict(object = roamer_fit, newdata = new_cats)
predicted_logit
predicted_probs <- predict(object = roamer_fit, newdata = new_cats, type = 'response')
predicted_probs
new_cats$predicted_prob <- predicted_probs
new_cats <- new_cats %>% mutate(roamer = ifelse(predicted_prob > 0.5, 'yes', 'no'))
ggplot(cats, aes(x = weight, y = wander_dist, color = roamer, group = roamer)) +
geom_point(size = 2) +
geom_point(data = new_cats, aes(x = weight, y = wander_dist, color = roamer), size = 8) +
theme_bw(base_size = 18)
cats <- cats %>%
mutate(scale_weight = scale(weight),
scale_wander = scale(wander_dist),
scale_age = scale(age))
cats_cluster <-
kmeans(x = cats %>% select(scale_weight, scale_wander, scale_age),
centers = 3,
nstart = 20)
str(cats_cluster)
cats_cluster$centers
cats$cluster <- factor(cats_cluster$cluster)
cluster_centers <- as.data.frame(cats_cluster$centers)
ggplot(data = cats) +
geom_point(aes(x = scale_age, y = scale_weight, color = cluster), size = 4) +
geom_point(data = cluster_centers, aes(x = scale_age, y = scale_weight), color = 'black', size  = 8) +
theme_bw(base_size = 18)
ggplot(data = cats) +
geom_point(aes(x = scale_age, y = scale_wander, color = cluster), size = 4) +
geom_point(data = cluster_centers, aes(x = scale_age, y = scale_wander), color = 'black', size  = 8) +
theme_bw(base_size = 18)
ggplot(data = cats) +
geom_point(aes(x = scale_weight, y = scale_wander, color = cluster), size = 4) +
geom_point(data = cluster_centers, aes(x = scale_weight, y = scale_wander), color = 'black', size  = 8) +
theme_bw(base_size = 18)
library("dplyr")
cats <- read.csv("data/herding-cats-small.csv")
cats[, "coat"]  ## usual way
select(cats, coat)
select(cats, sex)
select(cats, coat, cat_id, fixed) #maintains order
select(cats, coat, cat_id, fixed) #maintains order
cats[cats$coat == "black", ] ## usual way
filter(cats, coat == "black")
arrange(cats, coat)
# you can include additional columns to help sort the data
arrange(cats, coat, sex)
mutate(cats, weight = round(weight, 2))
mutate(cats, new_variable = age + weight)
mutate(cats, new_var_1 = age + weight, new_var_2 = age * weight)
# for example
sort(round(sqrt(cats$age * 2), 3))
(cats$age * 2) %>%
sqrt() %>%
round(3) %>%
sort()
## ----Another example --------------------------------------------------------------------
round(1.23456789, 3)
## ----Using pipe --------------------------------------------------------------------
1.23456789 %>% round(3)
3 %>% round(1.23456789, .)
select(filter(cats, coat == "black"), cat_id)
filter(cats, coat == "black") %>% select(cat_id)
cats %>%
filter(coat == "black") %>%
select(cat_id)
cats %>% summarize(mean_weight = mean(weight))
cats %>%
group_by(coat) %>%
summarize(mean_weight = mean(weight))
cats %>%
group_by(coat) %>%
mutate(centered_weight = weight - mean(weight))
## ----import data---------------------------------------------------------
library(dplyr)
cats <- read.csv("data/herding-cats.csv")
plot(x = cats$age, y = cats$weight)
install.packages("ggplot2")
ggplot(cats)
?geom_point
## ----load ggplot2--------------------------------------------------------
library(ggplot2)
ggplot(cats) +
geom_point()
## ---- scatterplot-------------------------------------------------
ggplot(cats) +
geom_point(aes(x = age, y = weight),
color = "red",
alpha = 0.5,
shape = 1,
size = 3)
ggplot(cats) +
geom_point(aes(x = age, y = weight)) +
scale_x_continuous(name = "Age",
breaks = c(1, 2, 3),
limits = c(-5, 15)) +
scale_y_continuous("Weight", trans = "log") +
ggtitle(label = "Scatterplot")
ggplot(cats) +
geom_point(aes(x = age, y = weight)) +
theme_bw()
ggplot(cats) +
geom_point(aes(x = age, y = weight)) +
xlab("Mother's age") +
ylab("Birth weight") +
facet_grid(. ~ coat) +
theme_linedraw()
ggsave("cats_facet.pdf", height = 5, width = 7)
dat <- read.table("data/airline-safety.csv",
sep = ",", header = TRUE)
ggplot(dat) +
geom_boxplot(aes(x = airline, y = avail_seat_km_per_week)) +
ggtitle("Available seats vs Airline") +
xlab("Airline") + ylab("Available Seats") +
theme_minimal()
ggplot(dat) +
geom_boxplot(aes(x = airline, y = avail_seat_km_per_week)) +
ggtitle("Available seats vs Airline") +
xlab("Airline") + ylab("Available Seats") +
theme_minimal()+
theme(axis.text = element_text(angle = 45, size = 5))
ggplot(dat) +
geom_point(aes(y = fatalities_00_14, x = avail_seat_km_per_week)) +
ggtitle("Fatalities vs. Available seats") +
xlab("Available Seats") + ylab("Fatalities") +
theme_minimal()+
theme(axis.text = element_text(angle = 45, size = 10))
# Recreate the above plot but add a straight line
