p.value <- 2*(pnorm(z_score))
p.value
wilcox.test(x,y, paired = FALSE, alternative = "two.sided", exact = FALSE, correct = TRUE)
U_X <- wilcox.test(x,y, paired = FALSE, alternative = "two.sided", exact = FALSE, correct = TRUE)$statistic
m <- length(x)
n <- length(y)
clt.mu <- m*n/2
clt.s2 <- m*n*(m+n+1)/12
z_score <- (U_X-clt.mu+1/2)/(sqrt(clt.s2)) # Uses continuity correction by subtracting 1/2
p.value <- 2*(pnorm(z_score))
p.value
D = 56489
R = 108231
(1+D/R)/2
D = 51382
R = 193975
(1+D/R)/2
D = 29861
R = 180944
(1+D/R)/2
?var.test
book1 = c(21,20,29,22,18,32)
book2 = c(45,14,35,58,41,64)
wilcox.test(book1,book2,paired=F,alternative="two.sided")
book1 = c(21,20,29,22,18,32)
book2 = c(45,14,35,58,41,64)
siegel.tukey(book1,book2,id.col=FALSE,adjust.median = FALSE,alternative = "two.sided")
age1 = c(14,9,15,13)
age2 = c(10,7,4,8,1,2)
age3 = c(12,6,3,11,5)
ages = c(14,9,15,13,10,7,4,8,1,2,12,6,3,11,5)
groups = (factor(rep(1:3, each=5)))
kruskal.test(ages,groups)
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("ALL", version = "3.8")
BiocManager::install("ALL", version = "4.0")
BiocManager::install("ALL", version = "3.12")
BiocManager::install(version = "3.12")
BiocManager::install("ALL", version = "3.12")
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = T)
## required for gene expression data classification example
if (!requireNamespace("BiocManager", quietly = TRUE))
install.packages("BiocManager")
## Caution: if you're doing this for the first time, it will install *many* packages.
# BiocManager::install(version = "3.12")
# BiocManager::install("ALL", version = "3.12")
library(ALL)
data(ALL)
dim(ALL)
data(ALL)
dim(ALL)
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F, cache = T)
## Caution: if you're doing this for the first time, it will install *many* packages.
# BiocManager::install(version = "3.12")
# BiocManager::install("ALL", version = "3.12")
library(ALL)
library(hdrm)
getwd()
downloadData(Scheetz2006)
attachData(Golub1999)
n = nrow(X)
# Split data into train and test sets
train_rows <- sample(1:n, n/2)
X.train <- X[train_rows, ]
X.test <- X[-train_rows, ]
y.train <- y[train_rows]
y.test <- y[-train_rows]
dim(X.train)
dim(X.test)
length(y.train)
length(y.test)
save.image(file = "Golub1999.RData")
library(Matrix)
set.seed(12345)
logit.inv <- function(x){
1/(1+exp(-x))
}
n=30;p=60
p1 = 5
beta <- c(rep(3,p1),rep(0,p-p1))
x=scale(matrix(rnorm(n*p),n,p))
eps=rnorm(n,mean=0,sd=0.1)
fx = x %*% beta
y=rbinom(n, logit.inv(fx+eps))
y=rbinom(n, prob= logit.inv(fx+eps))
y=rbinom(n, size = 1, prob= logit.inv(fx+eps))
n=200;p=50
p1 = 5
beta <- c(rep(3,p1),rep(0,p-p1))
x=scale(matrix(rnorm(n*p),n,p))
eps=rnorm(n,mean=0,sd=0.1)
fx = x %*% beta
y=rbinom(n, size = 1, prob= logit.inv(fx+eps))
train=sample(200,100)
lasso.fit <- glmnet(x = x, y = y, subset = train, family = "binomial") # binomial for binary response
library(glmnet)
lasso.fit <- glmnet(x = x, y = y, subset = train, family = "binomial") # binomial for binary response
plot(lasso.fit)
title("Fitted Coefficients",line=2.5)
cv.out <- cv.glmnet(x = x, y = y, subset = train, family = "binomial") # binomial for binary response
plot(cv.out)
title("Cross Validation",line=2.5)
(bestlam =cv.out$lambda.min)
lasso.coef=predict(cv.out,type ="coefficients",s=bestlam)
lasso.coef[lasso.coef !=0]
cv.out <- cv.glmnet(x = x, y = y, alpha = 1, subset = train, family = "binomial") # binomial for binary response
plot(cv.out)
title("Cross Validation",line=2.5)
(bestlam =cv.out$lambda.min)
lasso.coef=predict(cv.out,type ="coefficients",s=bestlam)
lasso.coef[lasso.coef !=0]
lasso.coef=predict(cv.out,type ="response",s=bestlam)
lasso.coef[lasso.coef !=0]
yhat=predict(cv.out,type ="response",s=bestlam)
yhat=predict(cv.out,type ="response",s=bestlam, newx = x[-train,])
yhat
y.pred=predict(cv.out,type ="class",s=bestlam, newx = x[-train,])
y.obs <- y[-train]
xtabs(~ y.obs +y.pred)
library(pacman)
p_load(xgboost)
y_train = y[train]
x_train = x[train,]
x_test = x[-train,]
?xgboost
#run XG Boost
fit_xgb <- xgboost(data.matrix(x[-train,]), y_train
, max_depth = 7
, eta = 0.02
, nthread = 2
, nrounds = 3000
, subsample = .7
, colsample_bytree = .7
, booster = "gbtree"
, eval_metric = "rmse"
, objective="binary:logistic")
#generating predctions
fishnetTest$pred.xgb <- predict(fit_xgb, data.matrix(x_test))
#generating predctions
pred.xgb <- predict(fit_xgb, data.matrix(x_test))
#show variable importance
importance_matrix1 <- xgb.importance(colnames(x_train), model = fit_xgb)
library(ggplot2)
var.imp.plot.xgb1 <-ggplot(data = importance_matrix1, aes(x=reorder(Feature,Gain), y=Gain, fill="#BD0026")) +
geom_bar(stat = "identity", colour = "black", size =1) +
scale_fill_manual (values = "#BD0026") +
labs(title = "XG Boost: Variable Importance",
x="Variable",
y="Variable Importance") +
theme(legend.position = "none")
var.imp.plot.xgb1 + coord_flip()
pred.xgb
?xgboost::predict.xgb.Booster
ypred.xgb = (pred.xgb>0.5)
xtabs(~ y.obs +ypred.xgb) ## Misclassification table.
#run XG Boost
fit_xgb <- xgboost(data.matrix(x[train,]), y_train
, max_depth = 7
, eta = 0.02
, nthread = 2
, nrounds = 3000
, subsample = .7
, colsample_bytree = .7
, booster = "gbtree"
, eval_metric = "rmse"
, objective="binary:logistic")
#generating predctions
pred.xgb <- predict(fit_xgb, data.matrix(x_test))
ypred.xgb = (pred.xgb>0.5)
xtabs(~ y.obs +ypred.xgb) ## Misclassification table.
#show variable importance
importance_matrix1 <- xgb.importance(colnames(x_train), model = fit_xgb)
library(ggplot2)
var.imp.plot.xgb1 <-ggplot(data = importance_matrix1, aes(x=reorder(Feature,Gain), y=Gain, fill="#BD0026")) +
geom_bar(stat = "identity", colour = "black", size =1) +
scale_fill_manual (values = "#BD0026") +
labs(title = "XG Boost: Variable Importance",
x="Variable",
y="Variable Importance") +
theme(legend.position = "none")
var.imp.plot.xgb1 + coord_flip()
var.imp.plot.xgb1 + coord_flip() + theme_minimal()
var.imp.plot.xgb1 <-ggplot(data = importance_matrix1, aes(x=reorder(Feature,Gain), y=Gain, fill="#BD0026")) +
geom_bar(stat = "identity", colour = "black", size =1) +
labs(title = "XG Boost: Variable Importance",
x="Variable",
y="Variable Importance") +
theme(legend.position = "none")
var.imp.plot.xgb1 + coord_flip() + theme_minimal()
var.imp.plot.xgb1 <-ggplot(data = importance_matrix1, aes(x=reorder(Feature,Gain), y=Gain)) +
geom_bar(stat = "identity", colour = "black", size =1) +
scale_fill_manual(values = "#BD0026") +
labs(title = "XG Boost: Variable Importance",
x="Variable",
y="Variable Importance") +
theme(legend.position = "none")
var.imp.plot.xgb1 + coord_flip() + theme_minimal()
var.imp.plot.xgb1 <-ggplot(data = importance_matrix1, aes(x=reorder(Feature,Gain), y=Gain)) +
geom_bar(stat = "identity",size =1) +
scale_fill_manual(values = "#BD0026") +
labs(title = "XG Boost: Variable Importance",
x="Variable",
y="Variable Importance") +
theme(legend.position = "none")
var.imp.plot.xgb1 + coord_flip() + theme_minimal()
lasso.coef <- predict(cv.out,type ="coefficient",s=bestlam)
plot(lasso.coef,pch=15,col="red", ylab = expression(beta), main = "Estimates")
points(beta,pch=15,col="blue")
plot(beta,pch=15,col="red", ylab = expression(beta), main = "Estimates")
points(lasso.coef,pch=15,col="blue")
plot(beta,pch=15,col="red", ylab = expression(beta), main = "Estimates vs true coef")
points(lasso.coef,pch=15,col="blue")
y=rbinom(n, size = 1, prob= logit.inv(fx))
train=sample(200,100) ## for supervised learning
library(glmnet)
## Cross validation to learn tuning parameter
cv.out <- cv.glmnet(x = x, y = y, alpha = 1, subset = train, family = "binomial") # binomial for binary response
plot(cv.out)
title("Cross Validation",line=2.5)
(bestlam =cv.out$lambda.min)
y.pred=predict(cv.out,type ="class",s=bestlam, newx = x[-train,])
y.obs <- y[-train]
xtabs(~ y.obs +y.pred) ## Misclassification table.
lasso.coef <- predict(cv.out,type ="coefficient",s=bestlam)
plot(beta,pch=15,col="red", ylab = expression(beta), main = "Estimates vs true coef")
points(lasso.coef,pch=15,col="blue")
library(pacman)
p_load(xgboost) ## or install then library
y_train = y[train]
x_train = x[train,]
x_test = x[-train,]
y=rbinom(n, size = 1, prob= logit.inv(fx+eps))
train=sample(200,100) ## for supervised learning
library(glmnet)
## Cross validation to learn tuning parameter
cv.out <- cv.glmnet(x = x, y = y, alpha = 1, subset = train, family = "binomial") # binomial for binary response
plot(cv.out)
title("Cross Validation",line=2.5)
(bestlam =cv.out$lambda.min)
y.pred=predict(cv.out,type ="class",s=bestlam, newx = x[-train,])
y.obs <- y[-train]
xtabs(~ y.obs +y.pred) ## Misclassification table.
lasso.coef <- predict(cv.out,type ="coefficient",s=bestlam)
plot(beta,pch=15,col="red", ylab = expression(beta), main = "Estimates vs true coef")
points(lasso.coef,pch=15,col="blue")
library(pacman)
p_load(xgboost) ## or install then library
y_train = y[train]
x_train = x[train,]
x_test = x[-train,]
#run XG Boost
fit_xgb <- xgboost(data.matrix(x[train,]), y_train
, max_depth = 7
, eta = 0.02
, nthread = 2
, nrounds = 3000
, subsample = .7
, colsample_bytree = .7
, booster = "gbtree"
, eval_metric = "rmse"
, objective="binary:logistic")
#generating predctions
pred.xgb <- predict(fit_xgb, data.matrix(x_test))
ypred.xgb = (pred.xgb>0.5)
xtabs(~ y.obs +ypred.xgb) ## Misclassification table.
#show variable importance
importance_matrix1 <- xgb.importance(colnames(x_train), model = fit_xgb)
library(ggplot2)
var.imp.plot.xgb1 <-ggplot(data = importance_matrix1, aes(x=reorder(Feature,Gain), y=Gain)) +
geom_bar(stat = "identity",size =1) +
scale_fill_manual(values = "#BD0026") +
labs(title = "XG Boost: Variable Importance",
x="Variable",
y="Variable Importance") +
theme(legend.position = "none")
var.imp.plot.xgb1 + coord_flip() + theme_minimal()
table(y)
x=matrix(rnorm(n*p),n,p)
x[1:180,]=x[1:180,]+2
eps=rnorm(n,mean=0,sd=0.1)
fx = x %*% beta
y=rbinom(n, size = 1, prob= logit.inv(fx+eps))
table(y)
x[1:150,]=x[1:150,]+2
## Generate data
n=200;p=50
p1 = 5
beta <- c(rep(3,p1),rep(0,p-p1)) ## First p1 beta non-zero, remaining p-p1 zeros.
x=matrix(rnorm(n*p),n,p)
x[1:150,]=x[1:150,]+2
eps=rnorm(n,mean=0,sd=0.1)
fx = x %*% beta
y=rbinom(n, size = 1, prob= logit.inv(fx+eps))
table(y)
## Generate data
n=200;p=50
p1 = 5
beta <- c(rep(3,p1),rep(0,p-p1)) ## First p1 beta non-zero, remaining p-p1 zeros.
x=matrix(rnorm(n*p),n,p)
x[1:100,]=x[1:100,]+2 ## Perturb first 100
eps=rnorm(n,mean=0,sd=0.1)
fx = x %*% beta
y=rbinom(n, size = 1, prob= logit.inv(fx+eps))
table(y)
table(y)
train=sample(200,100) ## for supervised learning
library(glmnet)
## Cross validation to learn tuning parameter
cv.out <- cv.glmnet(x = x, y = y, alpha = 1, subset = train, family = "binomial") # binomial for binary response
plot(cv.out)
title("Cross Validation",line=2.5)
(bestlam =cv.out$lambda.min)
y.pred=predict(cv.out,type ="class",s=bestlam, newx = x[-train,])
y.obs <- y[-train]
xtabs(~ y.obs +y.pred) ## Misclassification table.
lasso.coef <- predict(cv.out,type ="coefficient",s=bestlam)
plot(beta,pch=15,col="red", ylab = expression(beta), main = "Estimates vs true coef")
points(lasso.coef,pch=15,col="blue")
library(pacman)
p_load(xgboost) ## or install then library
y_train = y[train]
x_train = x[train,]
x_test = x[-train,]
#run XG Boost
fit_xgb <- xgboost(data.matrix(x[train,]), y_train
, max_depth = 7
, eta = 0.02
, nthread = 2
, nrounds = 3000
, subsample = .7
, colsample_bytree = .7
, booster = "gbtree"
, eval_metric = "rmse"
, objective="binary:logistic")
#generating predctions
pred.xgb <- predict(fit_xgb, data.matrix(x_test))
ypred.xgb = (pred.xgb>0.5)
xtabs(~ y.obs +ypred.xgb) ## Misclassification table.
#show variable importance
importance_matrix1 <- xgb.importance(colnames(x_train), model = fit_xgb)
library(ggplot2)
var.imp.plot.xgb1 <-ggplot(data = importance_matrix1, aes(x=reorder(Feature,Gain), y=Gain)) +
geom_bar(stat = "identity",size =1) +
scale_fill_manual(values = "#BD0026") +
labs(title = "XG Boost: Variable Importance",
x="Variable",
y="Variable Importance") +
theme(legend.position = "none")
var.imp.plot.xgb1 + coord_flip() + theme_minimal()
mat = matrix(rnorm(218*3),nrow=218)
kings<- mat[1:19,3] + 3
Robins<-mat[20:38,3]
Blues<-mat[40:58,3]
Waxs<-mat[60:78,3]
Bobos<-mat[80:98,3]
Cows<- mat[100:118,3]
Bunts<-mat[120:138,3]
Golds<-mat[140:158,3]
Savas<-mat[160:178,3]
Vesps<-mat[180:198,3]
Chips<-mat[200:218,3]
stem<- data.frame(kings,Robins,Blues,Waxs,Bobos,Cows,Bunts,Golds,Savas,Vesps,Chips )
#rownames(stem) <- letters[1:nrow(stem)]
boxplot(stem)
stamp<-stack(stem)
names(stamp)<-c("length","stem")
a1<-aov(length~stem,data=stamp)
summary(a1)
tk<-TukeyHSD(a1)
tk
plot(tk)
birds.fit<-aov(length~stem,data=stamp)
summary(birds.fit)
library(multcomp)
birds.mc <- glht(birds.fit, linfct = mcp(stem = "Tukey"))
install.packages("multcomp")
library(multcomp)
birds.mc <- glht(birds.fit, linfct = mcp(stem = "Tukey"))
birds.ci <- confint(birds.mc, level = 0.95)
birds.ci
plot(birds.ci, main = "", xlab = "stem length", cex.axis = 0.5)
row.names(birds.lmat) <- levels(stamp$ind)
library("HH")
t(birds.mc$linfct)
plot(birds.mmc2, x.offset = 8, main = "", main2 = "", cex.axis = 0.5)
birds.lmat <- cbind("Robins-kings" = c(-1, 1, rep(0,9)),
"Blues-kings" = c(-1,0, 1, rep(0,8)),
"Vesps-Chips" = c(rep(0,9),1,-1))
birds.mmc2 <- mmc(birds.fit, linfct = mcp(stem = "Tukey"),
focus.lmat = birds.lmat)
plotMatchMMC(birds.mmc2$mca, main = "", cex.axis = 0.5,
xlim = c(-5,10))
stem<- data.frame(kings,Robins,Blues,Waxs,Bobos,Cows,Bunts,Golds,Savas,Vesps,Chips )
rownames(stem) <- letters[1:nrow(stem)]
boxplot(stem)
stamp<-stack(stem)
names(stamp)<-c("length","stem")
a1<-aov(length~stem,data=stamp)
summary(a1)
tk<-TukeyHSD(a1)
tk
plot(tk)
cov_ind_districts = read.csv(url("https://api.covid19india.org/csv/latest/districts.csv"))
names(cov_ind_districts)
library(dplyr)
levels(cov_ind_districts$State)
table(cov_ind_districts$State)
3states_dist <- cov_ind_districts %>%
filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu"))
library(dplyr)
3states_dist <- cov_ind_districts %>% filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu"))
library(dplyr)
3states_dist <- cov_ind_districts %>% filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu"))
3states_dist = cov_ind_districts %>% filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu"))
str(cov_ind_districts)
?filter
3states_dist = cov_ind_districts %>% dplyr::filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu"))
3states_dist = cov_ind_districts %>% dplyr::filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu"))
3states_dist = cov_ind_districts %>% dplyr::filter(State == "Maharashtra")
cats <- read.csv("data/herding-cats-small.csv")
setwd("~/GitHub/DattaHub.github.io/stat5525")
cats <- read.csv("data/herding-cats-small.csv")
cats[, "coat"]  ## usual way
select(cats, coat)
select(cats, sex)
select(cats, coat, cat_id, fixed) #maintains order
cats$coat == "black"
cats[cats$coat == "black", ] ## usual way
filter(cats, coat == "black")
arrange(cats, coat)
# you can include additional columns to help sort the data
arrange(cats, coat, sex)
mutate(cats, weight = round(weight, 2))
mutate(cats, new_variable = age + weight)
mutate(cats, new_var_1 = age + weight, new_var_2 = age * weight)
# for example
sort(round(sqrt(cats$age * 2), 3))
(cats$age * 2) %>%
sqrt() %>%
round(3) %>%
sort()
## ----Another example --------------------------------------------------------------------
round(1.23456789, 3)
## ----Using pipe --------------------------------------------------------------------
1.23456789 %>% round(3)
3 %>% round(1.23456789, .)
select(filter(cats, coat == "black"), cat_id)
filter(cats, coat == "black") %>% select(cat_id)
names(cov_ind_districts)
3states_dist = cov_ind_districts %>%
dplyr::filter(State == "Maharashtra")
str(cov_ind_districts
)
cov_ind_districts %>%
dplyr::filter(State == "Maharashtra")
dat <- cov_ind_districts %>% dplyr::filter(State == "Maharashtra")
table(dat$State)
dat <- cov_ind_districts %>% dplyr::filter(State %in% c("Maharashtra", "Karnataka", "Tamil Nadu")
)
table(dat$State)
str(dat)
library(xlsx)
write.xlsx(dat, paste0("districtdata3states",Sys.Date(),".xlsx"))
library(rio)
write.xlsx(dat, paste0("districtdata3states",Sys.Date(),".xlsx"))
install.packages("xlsx")
library(xlsx)
write.xlsx(dat, paste0("districtdata3states",Sys.Date(),".xlsx"))
library(xlsx)
write.xlsx(dat, paste0("districtdata3states",Sys.Date(),".xlsx"))
library(readxl)
library(pacman)
p_load(RCurl, tidyverse, glue, DT, plyr, plotly, RColorBrewer)
p_load(janitor, tidyr, dplyr)
library(readxl)
write.xlsx(dat, paste0("districtdata3states",Sys.Date(),".xlsx"))
write.xlsx2(dat, paste0("districtdata3states",Sys.Date(),".xlsx"),sheetName="Sheet1",
col.names=TRUE, row.names=TRUE, append=FALSE)
install.packages("xlsx")
library(xlsx)
write.csv(dat, paste0("districtdata3states",Sys.Date(),".csv"))
getwd()
x = 3
y = 2
if ((x %% 2 != 0) & (y %% 2 == 0)) {
stop("Condition error occurred")
} else if (x > 3 & y <= 3) {
"Hello world!"
} else if (x > 3) {
"!dlrow olleH"
} else if (x <= 3) {
"Something else..."
}
x = 2
y = 2
if ((x %% 2 != 0) & (y %% 2 == 0)) {
stop("Condition error occurred")
} else if (x > 3 & y <= 3) {
"Hello world!"
} else if (x > 3) {
"!dlrow olleH"
} else if (x <= 3) {
"Something else..."
}
primes <- c(2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97)
x <- c(3, 4, 12, 19, 23, 48, 50, 61, 63, 78)
for (i in x) {
if (any(i == primes))
next
print(i)
}
